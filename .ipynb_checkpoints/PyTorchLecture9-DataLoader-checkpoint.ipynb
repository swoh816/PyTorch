{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Softmax Classifier\n",
    "\n",
    "For given number (logit) $z_j$ with index $j$, the softmax classifier $\\sigma(z_j)$ returns the probability of $z_j$ out of all numbers $z_i$ where $i\\in{N}$, and $N$ is the total number of indices.\n",
    "\n",
    "<center>$\\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^{N}e^{z_k}}$</center>\n",
    "\n",
    "The output of softmax is equivalent to the probability $p(j)$ of $j$:\n",
    "\n",
    "$$\\sigma(z_j) = p(j).$$\n",
    "\n",
    "For given $j$, we compute the loss by comparing the label with **one-hot label**, and we use **cross-entropy loss:**\n",
    "\n",
    "$$D(\\hat{Y_j}, Y_j) = -Y_j\\log\\hat{Y_j}$$\n",
    "\n",
    "where $\\hat{Y_j}$ is our predicted $p(j)$, and $Y_j$ is the one-hat label. Cross-entropy measures **differences between the distributions** of $Y$ and $\\hat{Y}$. Total loss is the sum of all cross entropies:\n",
    "\n",
    "$$L = \\sum_{j \\in N} D(\\hat{Y_j}, Y_j)$$\n",
    "\n",
    "## 9.1 Example: Cross entropy for 0, 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 =  0.356674943939\n",
      "loss2 =  2.30258509299\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([1,0,0])\n",
    "# One hot\n",
    "# 0: [1,0,0]\n",
    "# 1: [0,1,0]\n",
    "# 2: [0,0,1]\n",
    "\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1]) # CORRECT PREDICTION: has highest probability for 0\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6]) # WRONG PREDICTION: has highest probability for 2\n",
    "\n",
    "print(\"loss1 = \", np.sum(-Y*np.log(Y_pred1)))\n",
    "print(\"loss2 = \", np.sum(-Y*np.log(Y_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss1` has smaller loss than `loss2` because `Y_pred1` gives a better prediction.\n",
    "\n",
    "\n",
    "## 9.2 `CrossEntropyLoss()`: PyTorch implementation of Cross entropy\n",
    "\n",
    "In PyTorch, we can implement `CrossEntropyLoss()`. It is different from the way we calculated in the above cell:\n",
    "\n",
    "1. Label (`Y`) for `CrossEntropyLoss()` is **NOT ONE-HOT LABEL, BUT CLASS LABEL.**\n",
    "2. Predicted label (`Y_pred`) is **not softmax, but logit** ($z_j$ in the above description).\n",
    "\n",
    "In other words, `CrossEntropyLoss()` is the combination of softmax and cross entropy. (To be more precise, `CrossEntropyLoss` combines **`LogSoftMax` and `NLLLoss` in one single class**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 =  Variable containing:\n",
      " 0.4170\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss2 =  Variable containing:\n",
      " 1.8406\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tc.nn.CrossEntropyLoss()\n",
    "Y = Variable(tc.LongTensor([0]), requires_grad = False) # zeroth class\n",
    "Y_pred1 = Variable(tc.Tensor([[2.0, 1.0, 0.1]]))\n",
    "Y_pred2 = Variable(tc.Tensor([[0.5, 2.0, 0.3]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Loss1 = \", l1)\n",
    "print(\"Loss2 = \", l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 =  \n",
      " 0.7623\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss2 =  \n",
      " 1.0571\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tc.nn.CrossEntropyLoss()\n",
    "Y = Variable(tc.LongTensor([2, 0, 1]), requires_grad = False) # [2,0,1] corresponds to second, zeroth, first class.\n",
    "Y_pred1 = Variable(tc.Tensor([[0.1, 0.2, 0.9], [0.7, 0.2, 0.1], [0.3, 0.6, 0.1]]))\n",
    "Y_pred2 = Variable(tc.Tensor([[0.3, 0.3, 0.4], [0.4, 0.4, 0.2], [0.33, 0.33, 0.34]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Loss1 = \", l1.data)\n",
    "print(\"Loss2 = \", l2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `NLLLoss()` (**negative log-likelihood**) to use softmax classifier. It uses log-softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 =  \n",
      "-0.7333\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss2 =  \n",
      "-0.3767\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tc.nn.NLLLoss()\n",
    "Y = Variable(tc.LongTensor([2, 0, 1]), requires_grad = False)\n",
    "Y_pred1 = Variable(tc.Tensor([[0.1, 0.2, 0.9], [0.7, 0.2, 0.1], [0.3, 0.6, 0.1]]))\n",
    "Y_pred2 = Variable(tc.Tensor([[0.3, 0.3, 0.4], [0.4, 0.4, 0.2], [0.33, 0.33, 0.34]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Loss1 = \", l1.data)\n",
    "print(\"Loss2 = \", l2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Cross-entropy on MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = tc.nn.Linear(784, 520)\n",
    "        self.linear2 = tc.nn.Linear(520, 320)\n",
    "        self.linear3 = tc.nn.Linear(320, 240)\n",
    "        self.linear4 = tc.nn.Linear(240, 120)\n",
    "        self.linear5 = tc.nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) # YOU NEED TO FLATTEN THE VECTOR FIRST: (n, 1, 28, 28) -> (n, 784)\n",
    "        x = tc.nn.functional.relu(self.linear1(x))\n",
    "        x = tc.nn.functional.relu(self.linear2(x))\n",
    "        x = tc.nn.functional.relu(self.linear3(x))\n",
    "        x = tc.nn.functional.relu(self.linear4(x))\n",
    "        x = self.linear5(x) # We don't need activation here because we use LOGIT.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./Data/MNIST_Data/', train=True, download=True,\n",
    "               transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "# transforms.Compose: Composes several transforms together.\n",
    "# transforms.Normalize: Normalize an tensor image with mean and standard deviation.\n",
    "test_dataset = datasets.MNIST(root='./Data/MNIST_Data/', train=False,\n",
    "             transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "train_loader = tc.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=64)\n",
    "test_loader = tc.utils.data.DataLoader(dataset=test_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In line 2, you normalize with mean 0.1307 and standard deviation 0.3081. The mean and standard deviation are calculated from the entire distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1306604762738429\n",
      "0.30810780717887876\n"
     ]
    }
   ],
   "source": [
    "entire_train_data = train_loader.dataset.train_data.view(-1)\n",
    "entire_train_data = entire_train_data.type(tc.Tensor)\n",
    "print(entire_train_data.mean()/255) # 255 is the highest value of all elements (0~255).\n",
    "print(entire_train_data.std()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criterion = tc.nn.CrossEntropyLoss()\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 [0/60000 (0%)]\t Loss: 2.299086\n",
      "Train Epoch 0 [640/60000 (1%)]\t Loss: 2.285978\n",
      "Train Epoch 0 [1280/60000 (2%)]\t Loss: 2.313374\n",
      "Train Epoch 0 [1920/60000 (3%)]\t Loss: 2.299700\n",
      "Train Epoch 0 [2560/60000 (4%)]\t Loss: 2.292431\n",
      "Train Epoch 0 [3200/60000 (5%)]\t Loss: 2.288338\n",
      "Train Epoch 0 [3840/60000 (6%)]\t Loss: 2.290656\n",
      "Train Epoch 0 [4480/60000 (7%)]\t Loss: 2.289193\n",
      "Train Epoch 0 [5120/60000 (9%)]\t Loss: 2.271476\n",
      "Train Epoch 0 [5760/60000 (10%)]\t Loss: 2.274360\n",
      "Train Epoch 0 [6400/60000 (11%)]\t Loss: 2.281143\n",
      "Train Epoch 0 [7040/60000 (12%)]\t Loss: 2.267352\n",
      "Train Epoch 0 [7680/60000 (13%)]\t Loss: 2.273897\n",
      "Train Epoch 0 [8320/60000 (14%)]\t Loss: 2.276070\n",
      "Train Epoch 0 [8960/60000 (15%)]\t Loss: 2.262237\n",
      "Train Epoch 0 [9600/60000 (16%)]\t Loss: 2.252726\n",
      "Train Epoch 0 [10240/60000 (17%)]\t Loss: 2.232309\n",
      "Train Epoch 0 [10880/60000 (18%)]\t Loss: 2.225350\n",
      "Train Epoch 0 [11520/60000 (19%)]\t Loss: 2.207300\n",
      "Train Epoch 0 [12160/60000 (20%)]\t Loss: 2.231747\n",
      "Train Epoch 0 [12800/60000 (21%)]\t Loss: 2.212028\n",
      "Train Epoch 0 [13440/60000 (22%)]\t Loss: 2.202167\n",
      "Train Epoch 0 [14080/60000 (23%)]\t Loss: 2.174571\n",
      "Train Epoch 0 [14720/60000 (25%)]\t Loss: 2.164376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-350-d2426020abe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moutputs_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_tensor_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensor is not a torch image.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# TODO: make efficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36m_is_tensor_image\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_tensor_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, outputs) in enumerate(train_loader):\n",
    "        inputs, outputs = Variable(inputs), Variable(outputs)\n",
    "        outputs_pred = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs_pred, outputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'\n",
    "                  .format(epoch, batch_idx*len(inputs), len(train_loader.dataset),\n",
    "                  100.*batch_idx/len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would make the code more readable if you define `train` module and `test` module.\n",
    "\n",
    "When making `train` and `test` modules, **try to explicitly state your intent and set model.train() and model.eval() when necessary.**\n",
    "\n",
    "Explicitly indicating `model.train()` and `model.eval()` matters for some layers; some layers (like BatchNorm, Dropout) have different behavior during train/and evaluation so setting it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # SETS MODEL IN TRAINING MODEL\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, outputs) in enumerate(train_loader):\n",
    "        inputs, outputs = Variable(inputs), Variable(outputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs_pred = model(inputs)\n",
    "        loss = criterion(outputs_pred, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                 .format(epoch, batch_idx*len(inputs), len(train_loader.dataset), 100.*batch_idx/len(train_loader),\n",
    "                 loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # SETS MODEL IN EVALUATION MODEL\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        # volatile: Boolean indicating that the Variable should be used in\n",
    "        # inference mode, i.e. don't save the history. See\n",
    "        # :ref:`excluding-subgraphs` for more details.\n",
    "        # Can be changed only on leaf Variables.\n",
    "\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        # test_loss += criterion(output, target).data[0]\n",
    "        test_loss += criterion(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-prob\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        # What if you don't change the shape of target.data? Check at the bottom of the page.\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print ('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "           .format(test_loss, correct, len(test_loader.dataset), 100.*correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.202851\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 2.111411\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 2.087088\n",
      "Train Epoch: 0 [1920/60000 (3%)]\tLoss: 2.070420\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 1.984279\n",
      "Train Epoch: 0 [3200/60000 (5%)]\tLoss: 1.964135\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 1.844927\n",
      "Train Epoch: 0 [4480/60000 (7%)]\tLoss: 1.814125\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 1.721405\n",
      "Train Epoch: 0 [5760/60000 (10%)]\tLoss: 1.432234\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.489503\n",
      "Train Epoch: 0 [7040/60000 (12%)]\tLoss: 1.390342\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 1.324259\n",
      "Train Epoch: 0 [8320/60000 (14%)]\tLoss: 1.069510\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 1.135672\n",
      "Train Epoch: 0 [9600/60000 (16%)]\tLoss: 0.871543\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 0.867950\n",
      "Train Epoch: 0 [10880/60000 (18%)]\tLoss: 0.995606\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 0.699271\n",
      "Train Epoch: 0 [12160/60000 (20%)]\tLoss: 0.941294\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.806222\n",
      "Train Epoch: 0 [13440/60000 (22%)]\tLoss: 0.762445\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 0.726438\n",
      "Train Epoch: 0 [14720/60000 (25%)]\tLoss: 0.615405\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 0.706086\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.343771\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 0.706199\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 0.654478\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 0.484090\n",
      "Train Epoch: 0 [18560/60000 (31%)]\tLoss: 0.612176\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.766633\n",
      "Train Epoch: 0 [19840/60000 (33%)]\tLoss: 0.550210\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.517016\n",
      "Train Epoch: 0 [21120/60000 (35%)]\tLoss: 0.405297\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 0.447683\n",
      "Train Epoch: 0 [22400/60000 (37%)]\tLoss: 0.448492\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.474875\n",
      "Train Epoch: 0 [23680/60000 (39%)]\tLoss: 0.405432\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.279206\n",
      "Train Epoch: 0 [24960/60000 (42%)]\tLoss: 0.712243\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.393893\n",
      "Train Epoch: 0 [26240/60000 (44%)]\tLoss: 0.289678\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.440184\n",
      "Train Epoch: 0 [27520/60000 (46%)]\tLoss: 0.448188\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 0.463497\n",
      "Train Epoch: 0 [28800/60000 (48%)]\tLoss: 0.302099\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.346806\n",
      "Train Epoch: 0 [30080/60000 (50%)]\tLoss: 0.534220\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.371840\n",
      "Train Epoch: 0 [31360/60000 (52%)]\tLoss: 0.363657\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.412044\n",
      "Train Epoch: 0 [32640/60000 (54%)]\tLoss: 0.308958\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 0.568684\n",
      "Train Epoch: 0 [33920/60000 (57%)]\tLoss: 0.505295\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.371344\n",
      "Train Epoch: 0 [35200/60000 (59%)]\tLoss: 0.269537\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 0.536533\n",
      "Train Epoch: 0 [36480/60000 (61%)]\tLoss: 0.358250\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.408697\n",
      "Train Epoch: 0 [37760/60000 (63%)]\tLoss: 0.475230\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.343198\n",
      "Train Epoch: 0 [39040/60000 (65%)]\tLoss: 0.298540\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.526646\n",
      "Train Epoch: 0 [40320/60000 (67%)]\tLoss: 0.377716\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 0.381918\n",
      "Train Epoch: 0 [41600/60000 (69%)]\tLoss: 0.365564\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 0.281802\n",
      "Train Epoch: 0 [42880/60000 (71%)]\tLoss: 0.197142\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 0.435292\n",
      "Train Epoch: 0 [44160/60000 (74%)]\tLoss: 0.224284\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.315132\n",
      "Train Epoch: 0 [45440/60000 (76%)]\tLoss: 0.339236\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 0.255380\n",
      "Train Epoch: 0 [46720/60000 (78%)]\tLoss: 0.489485\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 0.281050\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.334959\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.405464\n",
      "Train Epoch: 0 [49280/60000 (82%)]\tLoss: 0.183915\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.343609\n",
      "Train Epoch: 0 [50560/60000 (84%)]\tLoss: 0.412099\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.384506\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.314369\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.185352\n",
      "Train Epoch: 0 [53120/60000 (88%)]\tLoss: 0.510466\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.470029\n",
      "Train Epoch: 0 [54400/60000 (91%)]\tLoss: 0.189402\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.257891\n",
      "Train Epoch: 0 [55680/60000 (93%)]\tLoss: 0.340376\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.463549\n",
      "Train Epoch: 0 [56960/60000 (95%)]\tLoss: 0.518102\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.370525\n",
      "Train Epoch: 0 [58240/60000 (97%)]\tLoss: 0.230057\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.332877\n",
      "Train Epoch: 0 [59520/60000 (99%)]\tLoss: 0.282787\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'size_average'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-353-c5a56344aef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-352-149f9467e50f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# test_loss += criterion(output, target).data[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# get the index of the max log-prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'size_average'"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.308537\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 2.311580\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 2.301602\n",
      "Train Epoch: 0 [1920/60000 (3%)]\tLoss: 2.299256\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 2.298642\n",
      "Train Epoch: 0 [3200/60000 (5%)]\tLoss: 2.301952\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 2.296908\n",
      "Train Epoch: 0 [4480/60000 (7%)]\tLoss: 2.296261\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 2.287950\n",
      "Train Epoch: 0 [5760/60000 (10%)]\tLoss: 2.293833\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.293328\n",
      "Train Epoch: 0 [7040/60000 (12%)]\tLoss: 2.293440\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 2.293146\n",
      "Train Epoch: 0 [8320/60000 (14%)]\tLoss: 2.287555\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 2.285890\n",
      "Train Epoch: 0 [9600/60000 (16%)]\tLoss: 2.287408\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 2.286450\n",
      "Train Epoch: 0 [10880/60000 (18%)]\tLoss: 2.281558\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 2.284967\n",
      "Train Epoch: 0 [12160/60000 (20%)]\tLoss: 2.282063\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.278095\n",
      "Train Epoch: 0 [13440/60000 (22%)]\tLoss: 2.279318\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 2.276715\n",
      "Train Epoch: 0 [14720/60000 (25%)]\tLoss: 2.274638\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 2.268690\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 2.278433\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 2.267386\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 2.267185\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 2.266528\n",
      "Train Epoch: 0 [18560/60000 (31%)]\tLoss: 2.266523\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.257988\n",
      "Train Epoch: 0 [19840/60000 (33%)]\tLoss: 2.250509\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 2.261645\n",
      "Train Epoch: 0 [21120/60000 (35%)]\tLoss: 2.254148\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 2.252691\n",
      "Train Epoch: 0 [22400/60000 (37%)]\tLoss: 2.236766\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 2.251339\n",
      "Train Epoch: 0 [23680/60000 (39%)]\tLoss: 2.227700\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 2.236430\n",
      "Train Epoch: 0 [24960/60000 (42%)]\tLoss: 2.213435\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.216441\n",
      "Train Epoch: 0 [26240/60000 (44%)]\tLoss: 2.208017\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 2.217017\n",
      "Train Epoch: 0 [27520/60000 (46%)]\tLoss: 2.196842\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 2.174520\n",
      "Train Epoch: 0 [28800/60000 (48%)]\tLoss: 2.168080\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 2.186541\n",
      "Train Epoch: 0 [30080/60000 (50%)]\tLoss: 2.152288\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 2.149651\n",
      "Train Epoch: 0 [31360/60000 (52%)]\tLoss: 2.136734\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.117189\n",
      "Train Epoch: 0 [32640/60000 (54%)]\tLoss: 2.090015\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 2.070105\n",
      "Train Epoch: 0 [33920/60000 (57%)]\tLoss: 2.076338\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 1.983762\n",
      "Train Epoch: 0 [35200/60000 (59%)]\tLoss: 2.045542\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 1.966132\n",
      "Train Epoch: 0 [36480/60000 (61%)]\tLoss: 2.035379\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 1.873402\n",
      "Train Epoch: 0 [37760/60000 (63%)]\tLoss: 1.888017\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.890228\n",
      "Train Epoch: 0 [39040/60000 (65%)]\tLoss: 1.778095\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 1.663010\n",
      "Train Epoch: 0 [40320/60000 (67%)]\tLoss: 1.666429\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 1.585061\n",
      "Train Epoch: 0 [41600/60000 (69%)]\tLoss: 1.505988\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 1.444868\n",
      "Train Epoch: 0 [42880/60000 (71%)]\tLoss: 1.452730\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 1.539730\n",
      "Train Epoch: 0 [44160/60000 (74%)]\tLoss: 1.412003\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.153782\n",
      "Train Epoch: 0 [45440/60000 (76%)]\tLoss: 1.151060\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 1.020744\n",
      "Train Epoch: 0 [46720/60000 (78%)]\tLoss: 1.070991\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 1.018182\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.898999\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.923508\n",
      "Train Epoch: 0 [49280/60000 (82%)]\tLoss: 0.837146\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.879964\n",
      "Train Epoch: 0 [50560/60000 (84%)]\tLoss: 0.767014\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.719855\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.870492\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.913915\n",
      "Train Epoch: 0 [53120/60000 (88%)]\tLoss: 0.849183\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.682258\n",
      "Train Epoch: 0 [54400/60000 (91%)]\tLoss: 0.579028\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.850215\n",
      "Train Epoch: 0 [55680/60000 (93%)]\tLoss: 0.597817\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.639786\n",
      "Train Epoch: 0 [56960/60000 (95%)]\tLoss: 0.719513\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.511458\n",
      "Train Epoch: 0 [58240/60000 (97%)]\tLoss: 0.734107\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.601453\n",
      "Train Epoch: 0 [59520/60000 (99%)]\tLoss: 0.640747\n",
      "\n",
      "Test set: Average loss: 0.6052, Accuracy: 8282/10000 (83%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.591495\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.607183\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.525070\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.643354\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.755866\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.570005\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.628954\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.675497\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.531909\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.618792\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.503376\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.434335\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.459969\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.460826\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.523228\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.552512\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.517553\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.745389\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.682882\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.437376\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.460923\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.583413\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.530663\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.367028\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.413156\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.306384\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.494684\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.700800\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.387771\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.543494\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.452034\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.647086\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.481558\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.518836\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.514664\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.524097\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.394271\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.622684\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.367697\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.611378\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.332793\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.236455\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.392988\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.617713\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.374828\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.226863\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.355340\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.444377\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.423197\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.273197\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.380052\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.468187\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.293922\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.440765\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.304615\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.553389\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.381543\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.532213\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.461114\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.285251\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.493324\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.451444\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.353584\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.432830\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.385782\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.509041\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.262517\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.303091\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.365720\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.275100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.194702\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.201029\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.343263\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.354013\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.331645\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.325003\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.318272\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.260135\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.184127\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.336632\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.331288\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.276151\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.334385\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.576735\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.405869\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.195392\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.307160\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.293672\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.287739\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.537750\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.368151\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.336795\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.444995\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.481134\n",
      "\n",
      "Test set: Average loss: 0.3211, Accuracy: 9030/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", download=True, train=True,\n",
    "                              transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "test_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=False,\n",
    "                             transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=64, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=64, num_workers=2)\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = tc.nn.Linear(28*28, 520)\n",
    "        self.linear2 = tc.nn.Linear(520, 420)\n",
    "        self.linear3 = tc.nn.Linear(420, 250)\n",
    "        self.linear4 = tc.nn.Linear(250, 100)\n",
    "        self.linear5 = tc.nn.Linear(100, 10)\n",
    "\n",
    "        self.relu = tc.nn.functional.relu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        l1 = self.relu(self.linear1(x))\n",
    "        l2 = self.relu(self.linear2(l1))\n",
    "        l3 = self.relu(self.linear3(l2))\n",
    "        l4 = self.relu(self.linear4(l3))\n",
    "        return self.linear5(l4)\n",
    "    \n",
    "model = Model()\n",
    "criterion = tc.nn.CrossEntropyLoss()\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, outputs) in enumerate(train_loader, 0):\n",
    "        inputs, outputs = Variable(inputs), Variable(outputs)\n",
    "        pred_outputs = model(inputs)\n",
    "        loss = criterion(pred_outputs, outputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                 .format(epoch, batch_idx*len(inputs), len(train_loader.dataset), 100.*batch_idx/len(train_loader),\n",
    "                 loss.data[0]))\n",
    "\n",
    "        \n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    for inputs, outputs in test_loader:\n",
    "        inputs, outputs = Variable(inputs, volatile=True), Variable(outputs)\n",
    "        pred_outputs = model(inputs)\n",
    "        test_loss += criterion(pred_outputs, outputs).data[0]\n",
    "        pred = pred_outputs.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(outputs.data.view_as(pred)).sum()\n",
    "        \n",
    "    test_loss /= len(test_loader)\n",
    "    print ('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "       .format(test_loss, correct, len(test_loader.dataset), 100.*correct/len(test_loader.dataset)))\n",
    "\n",
    "for epoch in range(2):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary\n",
    "\n",
    "## 1. What if you don't change the shape of target.data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 12 \n",
       "    1     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     1     1     1     0     0     0     0     0     0     0     1     0\n",
       "    0     0     0     0     0     0     1     1     0     0     0     0     0\n",
       "    0     1     1     1     0     0     0     0     0     0     0     1     0\n",
       "    0     0     0     0     1     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     1     0     0     0     0     1     0     1\n",
       "    1     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     1     1     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     1     1     0     0     0\n",
       "    0     0     0     0     0     0     0     0     1     1     0     0     0\n",
       "    0     0     0     0     0     1     0     0     0     0     1     0     1\n",
       "    0     1     1     1     0     0     0     0     0     0     0     1     0\n",
       "    0     0     0     0     0     1     0     0     0     0     1     0     1\n",
       "    0     0     0     0     1     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 13 to 15 \n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    1     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    0     0     0\n",
       "    1     0     0\n",
       "    0     1     0\n",
       "    0     0     1\n",
       "[torch.ByteTensor of size 16x16]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.eq(outputs.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the comparison between `pred` and `output`, which are different matrices (one is row and the other is column) but the same in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 9\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 6\n",
       " 7\n",
       " 8\n",
       "[torch.LongTensor of size 16]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "[torch.LongTensor of size 16x1]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.eq(outputs.data).numpy().diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1\n",
       "    1\n",
       "    0\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    0\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "[torch.ByteTensor of size 16x1]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.eq(outputs.data.view_as(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
