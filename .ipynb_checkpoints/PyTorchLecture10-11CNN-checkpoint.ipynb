{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optional caption text](Figures/CNNArchitecture.jpg)\n",
    "(http://parse.ele.tue.nl/cluster/2/CNNArchitecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Convolution\n",
    "\n",
    "Convolve the filter with the image (i.e., slide over the image spatially, computing **dot products**).\n",
    "\n",
    "We are going to look at only small portion of image at once; In other words, we look at **patch** via **filter (also called \"kernel\" or \"feature detector\")**.\n",
    "Each element in a filter is a weight. You can consider filter as a **weight vector for a patch.**\n",
    "\n",
    "We move filter at each time, and the length of the distance we move the filter is called **stride.**\n",
    "\n",
    "We can add **padding** by adding certain constant to the boundaries of images. It is often called **zero-padding** because we use zero as the value for the padding.\n",
    "\n",
    "As a result of application of filter to an image, you get a **feature map (or activation map).**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "After applying convolutional layer, apply **pooling layer** to reduce the amount of information. There are mainly two kinds of poolings:\n",
    "1. Max pooling\n",
    "2. Average pooling.\n",
    "\n",
    "### The advantage of using convolutional neural net:\n",
    "1. the size of weight matrix is smaller than fully connected net\n",
    "2. more flexible to handle images.\n",
    "![optional](Figures/cnn1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # You can also do\n",
    "        # super().__init__()\n",
    "        self.conv1 = tc.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = tc.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = tc.nn.MaxPool2d(2)\n",
    "        \n",
    "        self.relu = tc.nn.functional.relu\n",
    "        self.fc = tc.nn.Linear(320,10)\n",
    "        # self.fc = tc.nn.Linear(??,10)\n",
    "        # In order to determine what should go in ??, you can either use\n",
    "        ## 1. Error: If you put ??, you'll run into a RuntimeError:size mismatch, m1:[64x320], m2:[100x10]\n",
    "        ## and \"320\" is the number that should go into ??.\n",
    "        ## 2. x.size: You can print(x.size) and check the size of x.\n",
    "        self.log_softmax = tc.nn.functional.log_softmax\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(self.mp(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(self.mp(x))\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return self.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size() # [0]: Batch size, 1: Number of channels, 28*28: Input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=True,\n",
    "#                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                             transforms.Normalize((0.13066062, ), (0.30810776, ))]))\n",
    "test_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=False,\n",
    "                              transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                             transforms.Normalize((0.13066062, ), (0.30810776, ))]))\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle = True, batch_size=64, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# criterion = tc.nn.BCELoss()\n",
    "# optimizer = tc.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model = Net()\n",
    "criterion = tc.nn.CrossEntropyLoss()\n",
    "# CHOICE OF CRITERION IS VERY IMPORTANT!!!\n",
    "# criterion = tc.nn.BCELoss() does not work.\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        pred_label = model(data)\n",
    "        loss = criterion(pred_label, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                 .format(epoch, batch_idx*len(data), len(train_loader.dataset), 100.*batch_idx/len(train_loader),\n",
    "                 loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, label in test_loader:\n",
    "        data, label = Variable(data, volatile=True), Variable(label)\n",
    "        pred_label = model(data)\n",
    "        test_loss += criterion(pred_label, label).data[0]\n",
    "#         test_loss += tc.nn.functional.nll_loss(pred_label, label).data[0]\n",
    "                \n",
    "        pred = pred_label.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewook/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.330516\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 2.220621\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 2.125867\n",
      "Train Epoch: 0 [1920/60000 (3%)]\tLoss: 1.935391\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 1.508371\n",
      "Train Epoch: 0 [3200/60000 (5%)]\tLoss: 1.131037\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 0.966733\n",
      "Train Epoch: 0 [4480/60000 (7%)]\tLoss: 0.725739\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 0.490665\n",
      "Train Epoch: 0 [5760/60000 (10%)]\tLoss: 0.472078\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.480888\n",
      "Train Epoch: 0 [7040/60000 (12%)]\tLoss: 0.695810\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 0.377113\n",
      "Train Epoch: 0 [8320/60000 (14%)]\tLoss: 0.449468\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 0.488793\n",
      "Train Epoch: 0 [9600/60000 (16%)]\tLoss: 0.390571\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 0.210641\n",
      "Train Epoch: 0 [10880/60000 (18%)]\tLoss: 0.712056\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 0.358344\n",
      "Train Epoch: 0 [12160/60000 (20%)]\tLoss: 0.289750\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.326937\n",
      "Train Epoch: 0 [13440/60000 (22%)]\tLoss: 0.555935\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 0.430634\n",
      "Train Epoch: 0 [14720/60000 (25%)]\tLoss: 0.147671\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 0.204642\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.141355\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 0.325184\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 0.253296\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 0.231412\n",
      "Train Epoch: 0 [18560/60000 (31%)]\tLoss: 0.286670\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.295122\n",
      "Train Epoch: 0 [19840/60000 (33%)]\tLoss: 0.189701\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.236796\n",
      "Train Epoch: 0 [21120/60000 (35%)]\tLoss: 0.647015\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 0.180948\n",
      "Train Epoch: 0 [22400/60000 (37%)]\tLoss: 0.343793\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.453536\n",
      "Train Epoch: 0 [23680/60000 (39%)]\tLoss: 0.340335\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.192470\n",
      "Train Epoch: 0 [24960/60000 (42%)]\tLoss: 0.196851\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.082222\n",
      "Train Epoch: 0 [26240/60000 (44%)]\tLoss: 0.045997\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.164024\n",
      "Train Epoch: 0 [27520/60000 (46%)]\tLoss: 0.340875\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 0.159554\n",
      "Train Epoch: 0 [28800/60000 (48%)]\tLoss: 0.252323\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.092295\n",
      "Train Epoch: 0 [30080/60000 (50%)]\tLoss: 0.170384\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.247158\n",
      "Train Epoch: 0 [31360/60000 (52%)]\tLoss: 0.321345\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.201418\n",
      "Train Epoch: 0 [32640/60000 (54%)]\tLoss: 0.112719\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 0.334306\n",
      "Train Epoch: 0 [33920/60000 (57%)]\tLoss: 0.149801\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.326623\n",
      "Train Epoch: 0 [35200/60000 (59%)]\tLoss: 0.214235\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 0.234643\n",
      "Train Epoch: 0 [36480/60000 (61%)]\tLoss: 0.080448\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.247265\n",
      "Train Epoch: 0 [37760/60000 (63%)]\tLoss: 0.066823\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.098367\n",
      "Train Epoch: 0 [39040/60000 (65%)]\tLoss: 0.158219\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.132204\n",
      "Train Epoch: 0 [40320/60000 (67%)]\tLoss: 0.118493\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 0.101362\n",
      "Train Epoch: 0 [41600/60000 (69%)]\tLoss: 0.198166\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 0.240024\n",
      "Train Epoch: 0 [42880/60000 (71%)]\tLoss: 0.171213\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 0.130128\n",
      "Train Epoch: 0 [44160/60000 (74%)]\tLoss: 0.210252\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.108306\n",
      "Train Epoch: 0 [45440/60000 (76%)]\tLoss: 0.060409\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 0.117412\n",
      "Train Epoch: 0 [46720/60000 (78%)]\tLoss: 0.036164\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 0.096766\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.218328\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.076576\n",
      "Train Epoch: 0 [49280/60000 (82%)]\tLoss: 0.134456\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.211797\n",
      "Train Epoch: 0 [50560/60000 (84%)]\tLoss: 0.091712\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.129597\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.060603\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.209009\n",
      "Train Epoch: 0 [53120/60000 (88%)]\tLoss: 0.165393\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.214543\n",
      "Train Epoch: 0 [54400/60000 (91%)]\tLoss: 0.109651\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.122205\n",
      "Train Epoch: 0 [55680/60000 (93%)]\tLoss: 0.112245\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.102608\n",
      "Train Epoch: 0 [56960/60000 (95%)]\tLoss: 0.125710\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.044759\n",
      "Train Epoch: 0 [58240/60000 (97%)]\tLoss: 0.116184\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.280991\n",
      "Train Epoch: 0 [59520/60000 (99%)]\tLoss: 0.114057\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 9664/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.113089\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.039513\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.168313\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.166163\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.120594\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.046731\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.240828\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.099617\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.062538\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.026826\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.041743\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.093545\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.131412\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.024958\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.220002\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.125629\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.159327\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.110397\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.217604\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.136716\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.019392\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.231040\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.112018\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.114081\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.135823\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.058125\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.035540\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.022483\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.158872\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.170298\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.055875\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.144091\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.192783\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.045763\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.158195\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.057207\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.040953\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.068307\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.083104\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.104187\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.295089\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.116631\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.123524\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.120819\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.101373\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.103996\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.115339\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.056467\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.024559\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.110278\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.032311\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.213775\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.118992\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.072614\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.122054\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.036157\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.097201\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.084327\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.043545\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.049369\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.191377\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.124999\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.052921\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.130730\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.100848\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.026396\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.063356\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.069685\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.021367\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.031872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.141925\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.054255\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.046486\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.115104\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.053022\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.309232\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.085471\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.113151\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.088760\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.022616\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.038994\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.131400\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.140671\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.073617\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.107248\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.085924\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.042484\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.085894\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.045710\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.018368\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.083764\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.057814\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.119067\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.050395\n",
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 9734/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Advanced CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Inception modules\n",
    "\n",
    "When not know which size of filter to choose, **use all different filters at the same time.**\n",
    "\n",
    "You use $1\\times1$ filter because **it reduces the number of operations to a huge extent.**\n",
    "\n",
    "<img align=\"left\" src=\"./Figures/noreduce.png\" width=450>\n",
    "<img align=\"left\" src=\"./Figures/reduced1.png\"width=450>\n",
    "\n",
    "<!-- ![optional](./Figures/noreduce.png)![optional](./Figures/reduced1.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make an inception module\n",
    "<img align=\"center\" src=\"./Figures/inception_module.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InceptionA(tc.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        \n",
    "        self.branch_pool_1 = tc.nn.Conv2d(in_channels=in_channels, out_channels=24, kernel_size=1)\n",
    "        \n",
    "        self.branch1x1 = tc.nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        \n",
    "        self.branch5x5_1 = tc.nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        self.branch5x5_2 = tc.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.branch3x3_1 = tc.nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        self.branch3x3_2 = tc.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=3, padding=1)\n",
    "        self.branch3x3_3 = tc.nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, padding=1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch_pool = self.branch_pool_1(tc.nn.functional.avg_pool2d(x, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch5x5 = self.branch5x5_2(self.branch5x5_1(x))\n",
    "        branch3x3 = self.branch3x3_3(self.branch3x3_2(self.branch3x3_1(x)))\n",
    "        \n",
    "        outputs = [branch_pool, branch1x1, branch5x5, branch3x3]\n",
    "        return tc.cat(outputs, 1) # Concatenate output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = tc.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = tc.nn.Conv2d(88, 20, kernel_size=5)\n",
    "        \n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "        \n",
    "        self.mp = tc.nn.MaxPool2d(2)\n",
    "        self.fc = tc.nn.Linear(1408, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0) # size of batch\n",
    "        print(\"x.size1: \", x.size())\n",
    "        print(\"in_size: \", in_size)\n",
    "        x = tc.nn.functional.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = tc.nn.functional.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        print(\"x.size2: \", x.size())\n",
    "        x = self.fc(x)\n",
    "        return tc.nn.functional.log_softmax(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=True,\n",
    "           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.13066062, ), (0.30810776, ))]), download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=False,\n",
    "                              transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.13066062, ), (0.30810776, ))]))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=64, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=64, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = tc.nn.CrossEntropyLoss()\n",
    "# CHOICE OF CRITERION IS VERY IMPORTANT!!!\n",
    "# criterion = tc.nn.BCELoss() does not work.\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        pred_label = model(data)\n",
    "        loss = criterion(pred_label, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                 .format(epoch, batch_idx*len(data), len(train_loader.dataset), 100.*batch_idx/len(train_loader),\n",
    "                 loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    for data, label in test_loader:\n",
    "        data, label = Variable(data, volatile=True), Variable(label)\n",
    "        pred_label = model(data)\n",
    "        test_loss += criterion(pred_label, label).data[0]\n",
    "        \n",
    "        pred = pred_label.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.300683\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewook/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 2.292998\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 2.268855\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n",
      "x.size1:  torch.Size([64, 1, 28, 28])\n",
      "in_size:  64\n",
      "x.size2:  torch.Size([64, 1408])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-63:\n",
      "Traceback (most recent call last):\n",
      "Process Process-64:\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-6b333e7e5106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-524757e95836>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
