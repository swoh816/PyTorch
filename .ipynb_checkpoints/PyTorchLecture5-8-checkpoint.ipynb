{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Linear regression in the PyTorch way\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Rhythm:\n",
    "    1. Design your model using `class` with `Variable`\n",
    "    2. Construct loss and optimizer\n",
    "    3. Training cycle\n",
    "        (forward, backward, update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Design your model using `class` with `Variable`\n",
    "\n",
    "To design a **linear regression model** with PyTorch, we'll use a predefined function called **`tc.nn.Linear`.** `tc.nn.Linear` receives a Variable, and makes a prediction with a random weight. For example, we can construct a linear regression model with **one input and one output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to False, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
      "          additional dimensions\n",
      "        - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
      "          are the same shape as the input.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            (out_features x in_features)\n",
      "        bias:   the learnable bias of the module of shape (out_features)\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = autograd.Variable(torch.randn(128, 20))\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(tc.nn.Linear.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.1464\n",
       " 2.0271\n",
       " 2.9079\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.nn.Linear(1,1)(Variable(tc.Tensor([[1.0], [2.0], [3.0]]))) # (1,1): one input and one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear(in_features=1, out_features=1)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.nn.Linear.__repr__(tc.nn.Linear(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the (weight) parameter of `tc.nn.Linear` by using `.parameters()` (which is from a class `nn.Parameter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Parameter(Variable):\n",
      "    r\"\"\"A kind of Variable that is to be considered a module parameter.\n",
      "\n",
      "    Parameters are :class:`~torch.autograd.Variable` subclasses, that have a\n",
      "    very special property when used with :class:`Module` s - when they're\n",
      "    assigned as Module attributes they are automatically added to the list of\n",
      "    its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
      "    Assigning a Variable doesn't have such effect. This is because one might\n",
      "    want to cache some temporary state, like last hidden state of the RNN, in\n",
      "    the model. If there was no such class as :class:`Parameter`, these\n",
      "    temporaries would get registered too.\n",
      "\n",
      "    Another difference is that parameters can't be volatile and that they\n",
      "    require gradient by default.\n",
      "\n",
      "    Arguments:\n",
      "        data (Tensor): parameter tensor.\n",
      "        requires_grad (bool, optional): if the parameter requires gradient. See\n",
      "            :ref:`excluding-subgraphs` for more details.\n",
      "    \"\"\"\n",
      "    def __new__(cls, data=None, requires_grad=True):\n",
      "        return super(Parameter, cls).__new__(cls, data, requires_grad=requires_grad)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return 'Parameter containing:' + self.data.__repr__()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(tc.nn.Parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.5686\n",
       " [torch.FloatTensor of size 1x1], Parameter containing:\n",
       "  0.8710\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tc.nn.Linear(1,1).parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when you make a model, you'll make the model as a **subclass of `tc.nn.Module` to use all the functions therein.** Therefore, you need to put `tc.nn.Module` as an object of Model class by using `super`:\n",
    "\n",
    "`Model(tc.nn.Module):\n",
    "    def __init(self):\n",
    "        super(Model, self).__init__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from  torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(tc.Tensor([[1.0], [2.0], [3.0]])) # Define 3X1 dataset (matrix).\n",
    "y_data = Variable(tc.Tensor([[2.0], [4.0], [6.0]])) # Define 3X1 dataset (matrix).\n",
    "\n",
    "class Model(tc.nn.Module): # create class Model, which is a subclass of torch.nn.Module\n",
    "    def __init__(self): #Initialize class: Codes to be executed when the object is created\n",
    "        super(Model, self).__init__() #super lets the class call the parent classes (torch.nn.Module in this case) before all the codes below are executed.\n",
    "        # You can do some initalization task, or create some element and components for neural net.\n",
    "        self.linear = tc.nn.Linear(1, 1) # Linear model block: Input_size (1), Output_size (1)\n",
    "        \n",
    "    def forward(self, x): # one input (x)\n",
    "        # We don't use our own weight `w`, but we use the block we initialized in line 11.\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "            \n",
    "#Once created a class, you can create an instance using the class\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1107\n",
       " 0.2255\n",
       " 0.3403\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Construct loss (or select loss from the api) and optimizer\n",
    "\n",
    "Now we will calculate loss and do optimization. In order to to optimization, **you need to be able to use model parameter, which you can get by using `model.parameters()` (which is from a class `nn.Parameter`).**\n",
    "\n",
    "`nn.Parameter` class is a very very simple subclass of `Variable`, but it has a special behaviour when assigned as a module attribute - it gets added to the parameter list, and is returned when you call model.parameters(). Variables aren’t added automatically, we thought of many cases where you’d want to cache some Variable in your model, without adding them to parameters (e.g. when writing an recurrent network you might want to save the last hidden state so you don’t have to pass it over and over).\n",
    "\n",
    "Also Parameters are convenient, because **they `require_grad` by default.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.1892  0.1998  0.5011\n",
       " [torch.FloatTensor of size 1x3], Parameter containing:\n",
       " -0.5761\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tc.nn.Linear(3,1).parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  -9.5706\n",
       " [torch.FloatTensor of size 1x1], Parameter containing:\n",
       " -0.8476\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First element `list(model.parameters())[0]` corresponds to the input (and the size is the same as the input size), and the second element `list(model.parameters())[1]` corresponds to the output (and the size is the same as the output size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use MSE Loss\n",
    "criterion = tc.nn.MSELoss(size_average = False)\n",
    "                         \n",
    "# We use SGD. We need to pass what parameter / variable we are updating\n",
    "# In this case, we update model variable using model.parameters()\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Training cycle: Forward, loss, backward, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.049306869506836\n",
      "1 4.050587177276611\n",
      "2 1.8249815702438354\n",
      "3 0.8338918685913086\n",
      "4 0.39237838983535767\n",
      "5 0.19552509486675262\n",
      "6 0.10759192705154419\n",
      "7 0.06815139949321747\n",
      "8 0.05030248314142227\n",
      "9 0.04206973686814308\n",
      "10 0.03812200203537941\n",
      "11 0.0360858254134655\n",
      "12 0.03490469977259636\n",
      "13 0.03410810977220535\n",
      "14 0.033486586064100266\n",
      "15 0.032946910709142685\n",
      "16 0.03244740515947342\n",
      "17 0.03196948766708374\n",
      "18 0.03150486573576927\n",
      "19 0.03104979544878006\n",
      "20 0.03060251660645008\n",
      "21 0.030162280425429344\n",
      "22 0.02972857840359211\n",
      "23 0.029301246628165245\n",
      "24 0.02888011746108532\n",
      "25 0.028465040028095245\n",
      "26 0.028055943548679352\n",
      "27 0.027652747929096222\n",
      "28 0.027255330234766006\n",
      "29 0.026863649487495422\n",
      "30 0.026477541774511337\n",
      "31 0.02609703503549099\n",
      "32 0.0257219597697258\n",
      "33 0.025352314114570618\n",
      "34 0.024987956508994102\n",
      "35 0.02462884411215782\n",
      "36 0.024274900555610657\n",
      "37 0.023926017805933952\n",
      "38 0.023582151159644127\n",
      "39 0.023243235424160957\n",
      "40 0.022909199818968773\n",
      "41 0.022579971700906754\n",
      "42 0.0222554299980402\n",
      "43 0.021935606375336647\n",
      "44 0.021620357409119606\n",
      "45 0.021309612318873405\n",
      "46 0.021003365516662598\n",
      "47 0.020701516419649124\n",
      "48 0.02040400169789791\n",
      "49 0.02011076547205448\n",
      "50 0.019821763038635254\n",
      "51 0.019536901265382767\n",
      "52 0.01925608702003956\n",
      "53 0.018979357555508614\n",
      "54 0.01870660111308098\n",
      "55 0.018437759950757027\n",
      "56 0.018172793090343475\n",
      "57 0.017911607399582863\n",
      "58 0.017654210329055786\n",
      "59 0.017400486394762993\n",
      "60 0.017150426283478737\n",
      "61 0.01690395176410675\n",
      "62 0.016660982742905617\n",
      "63 0.01642155833542347\n",
      "64 0.01618555746972561\n",
      "65 0.015952961519360542\n",
      "66 0.01572365313768387\n",
      "67 0.015497715212404728\n",
      "68 0.01527497824281454\n",
      "69 0.01505542453378439\n",
      "70 0.014839059673249722\n",
      "71 0.014625806361436844\n",
      "72 0.014415601268410683\n",
      "73 0.014208439737558365\n",
      "74 0.014004262164235115\n",
      "75 0.013802996836602688\n",
      "76 0.013604617677628994\n",
      "77 0.013409093022346497\n",
      "78 0.013216395862400532\n",
      "79 0.013026420027017593\n",
      "80 0.012839224189519882\n",
      "81 0.012654723599553108\n",
      "82 0.01247282326221466\n",
      "83 0.012293585576117039\n",
      "84 0.012116890400648117\n",
      "85 0.011942753568291664\n",
      "86 0.011771139688789845\n",
      "87 0.01160196028649807\n",
      "88 0.011435218155384064\n",
      "89 0.011270883493125439\n",
      "90 0.011108892038464546\n",
      "91 0.010949247516691685\n",
      "92 0.010791880078613758\n",
      "93 0.010636807419359684\n",
      "94 0.010483925230801105\n",
      "95 0.010333254933357239\n",
      "96 0.010184739716351032\n",
      "97 0.010038360953330994\n",
      "98 0.009894099086523056\n",
      "99 0.009751907549798489\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    # Predict Y by passing every elements of x (input whole x matrix)\n",
    "    y_pred = model.forward(x_data)\n",
    "#     print(x_data)\n",
    "#     print(y_pred)\n",
    "    \n",
    "    #Compute loss: compare between predictive value and real value\n",
    "    loss = criterion(y_pred, x_data)\n",
    "    print(epoch, loss.data[0])\n",
    "    \n",
    "    #Backward pass\n",
    "    #3.1. Before we compute gradients, we initialize all gradients.\n",
    "    optimizer.zero_grad()\n",
    "    #3.2 Compute all gradients with respect to ALL VARIABLES\n",
    "    loss.backward()\n",
    "    #3.3 UPDATE variable. What variables? model.parameters().\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.0004\n",
      " 4.0001\n",
      " 5.9998\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = tc.nn.Linear(1,1)\n",
    "        \n",
    "    # YOU NEED TO NAME IT AS FORWARD.\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "x_data = Variable(tc.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(tc.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "criterion = tc.nn.MSELoss(size_average=False)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01)\n",
    "# Best option: optimizer = tc.optim.Rprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "print(model.forward(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.0004\n",
       " 4.0001\n",
       " 5.9998\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Logistic regression\n",
    "\n",
    "for **Binary prediction.** (Logistic regression is a **classification algorithm.**\n",
    "\n",
    "How to turn our model to predict 0 & 1 instead of real numbers? You can use **sigmoid function.** Sigmoid function **squashes real numbers between 0 and 1.**\n",
    "\n",
    "Therefore, if you want to turn real-valued function to binary (0 and 1), you can wrap the real-valued function with sigmoid:\n",
    "\n",
    "<center>$\\hat{y} = \\sigma(xw + b)$</center>\n",
    "where\n",
    "<center>$\\sigma(x)=\\frac{1}{e^{-x} + 1}$</center>\n",
    "\n",
    "Also, you need to use **cross entropy loss** instead of maximum square estimate (MSE), because MSE does not work well for sigmoid function.\n",
    "\n",
    "<center>$\\text{loss} = -\\frac{1}{N}\\sum_{n=1}^{N} y_n \\log(\\hat{y_n}) + (1-y_n)\\log(1-\\hat{y_n}) $</center>\n",
    "\n",
    "For binary-decision making, you set a threshold (cutoff value) that determines binary decision. The output of sigmoid function is between 0 and 1, and **it is common in the binary decision making that you consider the decision to be 1 if the output of sigmoid is larger than 0.5.** You can use different threshold, but then you need to be careful to decide the right threshold. About choosing the correct threshold, see\n",
    "\n",
    "1. <http://ethen8181.github.io/machine-learning/unbalanced/unbalanced.html>\n",
    "2. [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "x_data = Variable(tc.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(tc.Tensor([[0.0], [0.0], [1.0], [1.0]]))\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = tc.nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # wrap linear output with sigmoid function.\n",
    "        y_pred = tc.nn.functional.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = Model()\n",
    "# Binary cross-entropy loss\n",
    "criterion = tc.nn.BCELoss(size_average=True)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4705265462398529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8106\n",
       "[torch.FloatTensor of size 1x1]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loss.data[0])\n",
    "model(Variable(tc.Tensor([[4.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use different activation functions: `ReLU`, `SELU`, `Threshold`, `Tanh` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9745\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "x_data = Variable(tc.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(tc.Tensor([[0.0], [0.0], [1.0], [1.0]]))\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = tc.nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return tc.nn.functional.sigmoid(self.linear(x))\n",
    "        # Error if I try tanh or relu. why?\n",
    "    \n",
    "    \n",
    "model = Model()\n",
    "criterion = tc.nn.BCELoss(size_average=True)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(model(Variable(tc.Tensor([[7.0]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Go wide and deep\n",
    "\n",
    "## 1.5.1 Go Wide\n",
    "\n",
    "If we have more than one feature, we use **matrix multiplication** for our linear regression. For example, if we have two inputs and one output, we should use\n",
    "\n",
    "`linear = torch.nn.Linear(2,1)\n",
    "\n",
    "y_pred = linear(x_data)`\n",
    "\n",
    "Here, `x_data` has more than one column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.2 Go Deep\n",
    "\n",
    "We can use **multiple layers;** we can create multiple linear components, and connect them together.\n",
    "\n",
    "The problem of using sigmoid function is that it can incur **vanishing gradient problem.**\n",
    "\n",
    "How to solve? **Use other types of activation function (ReLU works really well)**\n",
    "\n",
    "![optional caption text](Figures/activation-functions.png)\n",
    "<https://rasbt.github.io/mlxtend/user_guide/general_concepts/activation-functions/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualizing activation functions, see <https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6488\n",
      " 0.6507\n",
      " 0.6490\n",
      " 0.6493\n",
      " 0.6489\n",
      " 0.6496\n",
      " 0.6500\n",
      " 0.6491\n",
      " 0.6489\n",
      " 0.6481\n",
      " 0.6495\n",
      " 0.6485\n",
      " 0.6479\n",
      " 0.6500\n",
      " 0.6495\n",
      " 0.6485\n",
      " 0.6494\n",
      " 0.6497\n",
      " 0.6496\n",
      " 0.6494\n",
      " 0.6485\n",
      " 0.6482\n",
      " 0.6494\n",
      " 0.6484\n",
      " 0.6486\n",
      " 0.6506\n",
      " 0.6475\n",
      " 0.6491\n",
      " 0.6489\n",
      " 0.6495\n",
      " 0.6504\n",
      " 0.6496\n",
      " 0.6488\n",
      " 0.6497\n",
      " 0.6486\n",
      " 0.6488\n",
      " 0.6503\n",
      " 0.6483\n",
      " 0.6493\n",
      " 0.6488\n",
      " 0.6493\n",
      " 0.6475\n",
      " 0.6489\n",
      " 0.6492\n",
      " 0.6501\n",
      " 0.6510\n",
      " 0.6497\n",
      " 0.6497\n",
      " 0.6505\n",
      " 0.6504\n",
      " 0.6496\n",
      " 0.6477\n",
      " 0.6487\n",
      " 0.6505\n",
      " 0.6484\n",
      " 0.6481\n",
      " 0.6483\n",
      " 0.6491\n",
      " 0.6494\n",
      " 0.6488\n",
      " 0.6500\n",
      " 0.6499\n",
      " 0.6490\n",
      " 0.6500\n",
      " 0.6492\n",
      " 0.6488\n",
      " 0.6504\n",
      " 0.6493\n",
      " 0.6500\n",
      " 0.6494\n",
      " 0.6479\n",
      " 0.6499\n",
      " 0.6509\n",
      " 0.6512\n",
      " 0.6492\n",
      " 0.6501\n",
      " 0.6492\n",
      " 0.6508\n",
      " 0.6511\n",
      " 0.6505\n",
      " 0.6490\n",
      " 0.6501\n",
      " 0.6487\n",
      " 0.6498\n",
      " 0.6481\n",
      " 0.6498\n",
      " 0.6475\n",
      " 0.6509\n",
      " 0.6498\n",
      " 0.6495\n",
      " 0.6483\n",
      " 0.6485\n",
      " 0.6488\n",
      " 0.6489\n",
      " 0.6508\n",
      " 0.6508\n",
      " 0.6497\n",
      " 0.6491\n",
      " 0.6496\n",
      " 0.6503\n",
      " 0.6483\n",
      " 0.6503\n",
      " 0.6502\n",
      " 0.6492\n",
      " 0.6499\n",
      " 0.6492\n",
      " 0.6498\n",
      " 0.6490\n",
      " 0.6494\n",
      " 0.6491\n",
      " 0.6499\n",
      " 0.6502\n",
      " 0.6488\n",
      " 0.6482\n",
      " 0.6492\n",
      " 0.6502\n",
      " 0.6507\n",
      " 0.6489\n",
      " 0.6480\n",
      " 0.6500\n",
      " 0.6499\n",
      " 0.6480\n",
      " 0.6494\n",
      " 0.6499\n",
      " 0.6494\n",
      " 0.6499\n",
      " 0.6493\n",
      " 0.6480\n",
      " 0.6494\n",
      " 0.6492\n",
      " 0.6493\n",
      " 0.6494\n",
      " 0.6502\n",
      " 0.6498\n",
      " 0.6482\n",
      " 0.6496\n",
      " 0.6491\n",
      " 0.6498\n",
      " 0.6489\n",
      " 0.6495\n",
      " 0.6501\n",
      " 0.6487\n",
      " 0.6498\n",
      " 0.6490\n",
      " 0.6492\n",
      " 0.6496\n",
      " 0.6480\n",
      " 0.6510\n",
      " 0.6496\n",
      " 0.6496\n",
      " 0.6481\n",
      " 0.6502\n",
      " 0.6481\n",
      " 0.6487\n",
      " 0.6495\n",
      " 0.6505\n",
      " 0.6502\n",
      " 0.6468\n",
      " 0.6493\n",
      " 0.6483\n",
      " 0.6491\n",
      " 0.6498\n",
      " 0.6489\n",
      " 0.6492\n",
      " 0.6504\n",
      " 0.6496\n",
      " 0.6498\n",
      " 0.6497\n",
      " 0.6493\n",
      " 0.6493\n",
      " 0.6509\n",
      " 0.6499\n",
      " 0.6499\n",
      " 0.6482\n",
      " 0.6492\n",
      " 0.6480\n",
      " 0.6486\n",
      " 0.6491\n",
      " 0.6496\n",
      " 0.6494\n",
      " 0.6491\n",
      " 0.6501\n",
      " 0.6492\n",
      " 0.6489\n",
      " 0.6482\n",
      " 0.6490\n",
      " 0.6488\n",
      " 0.6493\n",
      " 0.6492\n",
      " 0.6482\n",
      " 0.6490\n",
      " 0.6483\n",
      " 0.6498\n",
      " 0.6489\n",
      " 0.6495\n",
      " 0.6502\n",
      " 0.6494\n",
      " 0.6498\n",
      " 0.6490\n",
      " 0.6498\n",
      " 0.6498\n",
      " 0.6501\n",
      " 0.6484\n",
      " 0.6502\n",
      " 0.6477\n",
      " 0.6481\n",
      " 0.6491\n",
      " 0.6487\n",
      " 0.6510\n",
      " 0.6489\n",
      " 0.6480\n",
      " 0.6490\n",
      " 0.6486\n",
      " 0.6481\n",
      " 0.6494\n",
      " 0.6491\n",
      " 0.6503\n",
      " 0.6492\n",
      " 0.6486\n",
      " 0.6481\n",
      " 0.6493\n",
      " 0.6480\n",
      " 0.6503\n",
      " 0.6500\n",
      " 0.6494\n",
      " 0.6501\n",
      " 0.6501\n",
      " 0.6488\n",
      " 0.6496\n",
      " 0.6486\n",
      " 0.6502\n",
      " 0.6497\n",
      " 0.6500\n",
      " 0.6494\n",
      " 0.6492\n",
      " 0.6492\n",
      " 0.6495\n",
      " 0.6500\n",
      " 0.6498\n",
      " 0.6502\n",
      " 0.6496\n",
      " 0.6494\n",
      " 0.6486\n",
      " 0.6486\n",
      " 0.6496\n",
      " 0.6492\n",
      " 0.6507\n",
      " 0.6490\n",
      " 0.6499\n",
      " 0.6502\n",
      " 0.6499\n",
      " 0.6491\n",
      " 0.6496\n",
      " 0.6502\n",
      " 0.6507\n",
      " 0.6506\n",
      " 0.6493\n",
      " 0.6499\n",
      " 0.6504\n",
      " 0.6489\n",
      " 0.6495\n",
      " 0.6490\n",
      " 0.6493\n",
      " 0.6502\n",
      " 0.6488\n",
      " 0.6499\n",
      " 0.6488\n",
      " 0.6489\n",
      " 0.6494\n",
      " 0.6486\n",
      " 0.6480\n",
      " 0.6493\n",
      " 0.6501\n",
      " 0.6495\n",
      " 0.6487\n",
      " 0.6509\n",
      " 0.6490\n",
      " 0.6483\n",
      " 0.6489\n",
      " 0.6483\n",
      " 0.6491\n",
      " 0.6494\n",
      " 0.6495\n",
      " 0.6501\n",
      " 0.6489\n",
      " 0.6480\n",
      " 0.6491\n",
      " 0.6494\n",
      " 0.6499\n",
      " 0.6480\n",
      " 0.6491\n",
      " 0.6499\n",
      " 0.6492\n",
      " 0.6480\n",
      " 0.6483\n",
      " 0.6490\n",
      " 0.6498\n",
      " 0.6488\n",
      " 0.6492\n",
      " 0.6494\n",
      " 0.6494\n",
      " 0.6481\n",
      " 0.6485\n",
      " 0.6495\n",
      " 0.6498\n",
      " 0.6498\n",
      " 0.6492\n",
      " 0.6497\n",
      " 0.6503\n",
      " 0.6492\n",
      " 0.6499\n",
      " 0.6499\n",
      " 0.6494\n",
      " 0.6495\n",
      " 0.6480\n",
      " 0.6500\n",
      " 0.6503\n",
      " 0.6495\n",
      " 0.6474\n",
      " 0.6494\n",
      " 0.6500\n",
      " 0.6498\n",
      " 0.6484\n",
      " 0.6496\n",
      " 0.6489\n",
      " 0.6493\n",
      " 0.6502\n",
      " 0.6492\n",
      " 0.6484\n",
      " 0.6505\n",
      " 0.6486\n",
      " 0.6488\n",
      " 0.6490\n",
      " 0.6485\n",
      " 0.6484\n",
      " 0.6503\n",
      " 0.6498\n",
      " 0.6507\n",
      " 0.6493\n",
      " 0.6483\n",
      " 0.6478\n",
      " 0.6503\n",
      " 0.6502\n",
      " 0.6501\n",
      " 0.6496\n",
      " 0.6497\n",
      " 0.6495\n",
      " 0.6498\n",
      " 0.6505\n",
      " 0.6490\n",
      " 0.6480\n",
      " 0.6499\n",
      " 0.6485\n",
      " 0.6477\n",
      " 0.6494\n",
      " 0.6494\n",
      " 0.6481\n",
      " 0.6482\n",
      " 0.6480\n",
      " 0.6496\n",
      " 0.6495\n",
      " 0.6495\n",
      " 0.6493\n",
      " 0.6501\n",
      " 0.6489\n",
      " 0.6496\n",
      " 0.6483\n",
      " 0.6484\n",
      " 0.6497\n",
      " 0.6497\n",
      " 0.6474\n",
      " 0.6495\n",
      " 0.6501\n",
      " 0.6492\n",
      " 0.6482\n",
      " 0.6500\n",
      " 0.6502\n",
      " 0.6497\n",
      " 0.6504\n",
      " 0.6501\n",
      " 0.6504\n",
      " 0.6498\n",
      " 0.6486\n",
      " 0.6484\n",
      " 0.6498\n",
      " 0.6496\n",
      " 0.6492\n",
      " 0.6499\n",
      " 0.6495\n",
      " 0.6493\n",
      " 0.6504\n",
      " 0.6495\n",
      " 0.6496\n",
      " 0.6503\n",
      " 0.6498\n",
      " 0.6499\n",
      " 0.6486\n",
      " 0.6495\n",
      " 0.6489\n",
      " 0.6499\n",
      " 0.6491\n",
      " 0.6497\n",
      " 0.6485\n",
      " 0.6500\n",
      " 0.6496\n",
      " 0.6501\n",
      " 0.6502\n",
      " 0.6489\n",
      " 0.6481\n",
      " 0.6500\n",
      " 0.6511\n",
      " 0.6494\n",
      " 0.6505\n",
      " 0.6497\n",
      " 0.6495\n",
      " 0.6492\n",
      " 0.6478\n",
      " 0.6497\n",
      " 0.6485\n",
      " 0.6490\n",
      " 0.6496\n",
      " 0.6492\n",
      " 0.6484\n",
      " 0.6495\n",
      " 0.6505\n",
      " 0.6496\n",
      " 0.6505\n",
      " 0.6498\n",
      " 0.6509\n",
      " 0.6491\n",
      " 0.6484\n",
      " 0.6495\n",
      " 0.6502\n",
      " 0.6493\n",
      " 0.6485\n",
      " 0.6502\n",
      " 0.6497\n",
      " 0.6493\n",
      " 0.6505\n",
      " 0.6476\n",
      " 0.6502\n",
      " 0.6487\n",
      " 0.6491\n",
      " 0.6492\n",
      " 0.6507\n",
      " 0.6501\n",
      " 0.6494\n",
      " 0.6486\n",
      " 0.6501\n",
      " 0.6484\n",
      " 0.6489\n",
      " 0.6497\n",
      " 0.6468\n",
      " 0.6483\n",
      " 0.6507\n",
      " 0.6486\n",
      " 0.6498\n",
      " 0.6488\n",
      " 0.6486\n",
      " 0.6499\n",
      " 0.6491\n",
      " 0.6491\n",
      " 0.6487\n",
      " 0.6499\n",
      " 0.6496\n",
      " 0.6499\n",
      " 0.6484\n",
      " 0.6500\n",
      " 0.6483\n",
      " 0.6495\n",
      " 0.6493\n",
      " 0.6484\n",
      " 0.6486\n",
      " 0.6495\n",
      " 0.6492\n",
      " 0.6499\n",
      " 0.6492\n",
      " 0.6490\n",
      " 0.6490\n",
      " 0.6494\n",
      " 0.6476\n",
      " 0.6505\n",
      " 0.6474\n",
      " 0.6500\n",
      " 0.6498\n",
      " 0.6499\n",
      " 0.6491\n",
      " 0.6504\n",
      " 0.6479\n",
      " 0.6497\n",
      " 0.6503\n",
      " 0.6478\n",
      " 0.6488\n",
      " 0.6488\n",
      " 0.6503\n",
      " 0.6493\n",
      " 0.6489\n",
      " 0.6496\n",
      " 0.6489\n",
      " 0.6481\n",
      " 0.6493\n",
      " 0.6493\n",
      " 0.6480\n",
      " 0.6488\n",
      " 0.6486\n",
      " 0.6484\n",
      " 0.6505\n",
      " 0.6502\n",
      " 0.6495\n",
      " 0.6477\n",
      " 0.6487\n",
      " 0.6488\n",
      " 0.6501\n",
      " 0.6495\n",
      " 0.6497\n",
      " 0.6485\n",
      " 0.6501\n",
      " 0.6500\n",
      " 0.6494\n",
      " 0.6500\n",
      " 0.6494\n",
      " 0.6494\n",
      " 0.6502\n",
      " 0.6493\n",
      " 0.6494\n",
      " 0.6497\n",
      " 0.6502\n",
      " 0.6499\n",
      " 0.6485\n",
      " 0.6484\n",
      " 0.6491\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6498\n",
      " 0.6484\n",
      " 0.6495\n",
      " 0.6498\n",
      " 0.6482\n",
      " 0.6481\n",
      " 0.6497\n",
      " 0.6482\n",
      " 0.6490\n",
      " 0.6497\n",
      " 0.6499\n",
      " 0.6480\n",
      " 0.6503\n",
      " 0.6502\n",
      " 0.6490\n",
      " 0.6503\n",
      " 0.6483\n",
      " 0.6486\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6487\n",
      " 0.6500\n",
      " 0.6495\n",
      " 0.6494\n",
      " 0.6505\n",
      " 0.6488\n",
      " 0.6488\n",
      " 0.6490\n",
      " 0.6489\n",
      " 0.6497\n",
      " 0.6489\n",
      " 0.6498\n",
      " 0.6504\n",
      " 0.6502\n",
      " 0.6497\n",
      " 0.6496\n",
      " 0.6489\n",
      " 0.6488\n",
      " 0.6473\n",
      " 0.6480\n",
      " 0.6502\n",
      " 0.6482\n",
      " 0.6488\n",
      " 0.6492\n",
      " 0.6514\n",
      " 0.6487\n",
      " 0.6497\n",
      " 0.6483\n",
      " 0.6499\n",
      " 0.6483\n",
      " 0.6494\n",
      " 0.6491\n",
      " 0.6504\n",
      " 0.6490\n",
      " 0.6491\n",
      " 0.6487\n",
      " 0.6496\n",
      " 0.6493\n",
      " 0.6506\n",
      " 0.6508\n",
      " 0.6499\n",
      " 0.6502\n",
      " 0.6479\n",
      " 0.6492\n",
      " 0.6496\n",
      " 0.6497\n",
      " 0.6502\n",
      " 0.6488\n",
      " 0.6506\n",
      " 0.6502\n",
      " 0.6494\n",
      " 0.6484\n",
      " 0.6500\n",
      " 0.6478\n",
      " 0.6500\n",
      " 0.6493\n",
      " 0.6505\n",
      " 0.6489\n",
      " 0.6495\n",
      " 0.6494\n",
      " 0.6508\n",
      " 0.6482\n",
      " 0.6482\n",
      " 0.6493\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6483\n",
      " 0.6488\n",
      " 0.6497\n",
      " 0.6494\n",
      " 0.6489\n",
      " 0.6504\n",
      " 0.6503\n",
      " 0.6493\n",
      " 0.6485\n",
      " 0.6490\n",
      " 0.6501\n",
      " 0.6489\n",
      " 0.6502\n",
      " 0.6492\n",
      " 0.6498\n",
      " 0.6485\n",
      " 0.6500\n",
      " 0.6499\n",
      " 0.6498\n",
      " 0.6497\n",
      " 0.6490\n",
      " 0.6480\n",
      " 0.6501\n",
      " 0.6504\n",
      " 0.6500\n",
      " 0.6490\n",
      " 0.6502\n",
      " 0.6502\n",
      " 0.6506\n",
      " 0.6501\n",
      " 0.6490\n",
      " 0.6497\n",
      " 0.6478\n",
      " 0.6498\n",
      " 0.6477\n",
      " 0.6479\n",
      " 0.6494\n",
      " 0.6496\n",
      " 0.6485\n",
      " 0.6492\n",
      " 0.6490\n",
      " 0.6480\n",
      " 0.6484\n",
      " 0.6503\n",
      " 0.6480\n",
      " 0.6494\n",
      " 0.6479\n",
      " 0.6490\n",
      " 0.6480\n",
      " 0.6496\n",
      " 0.6501\n",
      " 0.6508\n",
      " 0.6504\n",
      " 0.6491\n",
      " 0.6491\n",
      " 0.6497\n",
      " 0.6479\n",
      " 0.6500\n",
      " 0.6502\n",
      " 0.6509\n",
      " 0.6501\n",
      " 0.6485\n",
      " 0.6492\n",
      " 0.6475\n",
      " 0.6497\n",
      " 0.6481\n",
      " 0.6504\n",
      " 0.6492\n",
      " 0.6494\n",
      " 0.6498\n",
      " 0.6497\n",
      " 0.6497\n",
      " 0.6499\n",
      " 0.6490\n",
      " 0.6491\n",
      " 0.6494\n",
      " 0.6497\n",
      " 0.6499\n",
      " 0.6491\n",
      " 0.6507\n",
      " 0.6482\n",
      " 0.6501\n",
      " 0.6504\n",
      " 0.6487\n",
      " 0.6489\n",
      " 0.6488\n",
      " 0.6498\n",
      " 0.6492\n",
      " 0.6490\n",
      " 0.6489\n",
      " 0.6499\n",
      " 0.6492\n",
      " 0.6502\n",
      " 0.6490\n",
      " 0.6492\n",
      " 0.6486\n",
      " 0.6493\n",
      " 0.6494\n",
      " 0.6500\n",
      " 0.6497\n",
      " 0.6498\n",
      " 0.6505\n",
      " 0.6493\n",
      " 0.6494\n",
      " 0.6491\n",
      " 0.6503\n",
      " 0.6492\n",
      " 0.6503\n",
      " 0.6481\n",
      " 0.6496\n",
      " 0.6493\n",
      " 0.6496\n",
      " 0.6477\n",
      " 0.6502\n",
      " 0.6505\n",
      " 0.6482\n",
      " 0.6476\n",
      " 0.6479\n",
      " 0.6497\n",
      " 0.6494\n",
      " 0.6491\n",
      " 0.6486\n",
      " 0.6499\n",
      " 0.6494\n",
      " 0.6506\n",
      " 0.6490\n",
      " 0.6487\n",
      " 0.6490\n",
      " 0.6489\n",
      " 0.6483\n",
      " 0.6501\n",
      " 0.6475\n",
      " 0.6501\n",
      " 0.6485\n",
      " 0.6494\n",
      " 0.6474\n",
      " 0.6504\n",
      " 0.6495\n",
      " 0.6495\n",
      " 0.6508\n",
      "[torch.FloatTensor of size 759x1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewook/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([759])) that is different to the input size (torch.Size([759, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xy = np.loadtxt('./Data/diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x = Variable(tc.from_numpy(xy[:,0:-1])) # x: 759*8\n",
    "y = Variable(tc.from_numpy(xy[:,-1])) # y: 759*1\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = tc.nn.Linear(8,6) # Layer 1\n",
    "        self.l2 = tc.nn.Linear(6,4) # Layer 2\n",
    "        self.l3 = tc.nn.Linear(4,1) # Layer 3\n",
    "        self.sigmoid = tc.nn.functional.sigmoid\n",
    "        #self.sigmoid = tc.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x)) # Layer 1\n",
    "        out2 = self.sigmoid(self.l2(out1)) # Layer 2\n",
    "        out3 = self.sigmoid(self.l3(out2)) # Layer 3\n",
    "        return out3\n",
    "        \n",
    "model = Model()\n",
    "\n",
    "criterion = tc.nn.BCELoss(size_average=True)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 DataLoader\n",
    "\n",
    "If the data size is large, we cannot feed all data into model.\n",
    "\n",
    "In such case, we divide dataset into small batches, and then we compute gradients, and then we update weights.\n",
    "\n",
    "**one epoch = one forward pass and one backward pass** of all the training examples\n",
    "\n",
    "**batch size = the number of training examples in one forward/backward pass.** The higher the batch size, the more memory space you'll need.\n",
    "\n",
    "**number of iterations = number of passes**, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "\n",
    "Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n",
    "(<https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks>)\n",
    "\n",
    "About batch size vs number of iterations, see\n",
    "<https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network>\n",
    "\n",
    "and\n",
    "\n",
    "<https://www.quora.com/In-deep-learning-why-dont-we-use-the-whole-training-set-to-compute-the-gradient>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.1 Make Custom DataLoader\n",
    "\n",
    "We need to make methods under DataLoader:\n",
    "\n",
    "1. `getitem` for given index which has written our item\n",
    "2. the size of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self): #download, read data, etc.\n",
    "        xy = np.loadtxt('./Data/diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = tc.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = tc.from_numpy(xy[:, [-1]]) # NOTE xy[:, -1] IS NOT APPROPRIATE FOR THIS CASE\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index): # return one item on the index\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__(self): # return the size of data (data length)\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DiabetesDataset() # Make instance of Diabete dataset\n",
    "\n",
    "# We feed dataset into DataLoader.\n",
    "train_loader = DataLoader(dataset = dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "# Decide batch size, and whether to shuffle.\n",
    "# num_workers determine the number of multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data # data[0]: 32 inputs, data[1]: 32 labels\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        label_pred = model(inputs)\n",
    "        loss = criterion(label_pred, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3623938262462616"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.2 Use different dataset loaders\n",
    "\n",
    "### 1.6.2.1 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "#MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./Data/MNIST_Data', train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST(root='./Data/MNIST_Data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADolJREFUeJzt3X2MXOV1x/HfyXq9jo1JvHVsHOJgxzgBYhqTjgzICFwh\nXKdCMqgCYkWRQ5M4LzgprStBraq4FancKiF1CUVamq1tifcEiv+gSZAVAVFhy+IQXuLwErMli7e7\nmA3YEOKX3dM/9m60MTvPrGfuzJ3d8/1I1szcc+/co4Hf3pl55t7H3F0A4nlP0Q0AKAbhB4Ii/EBQ\nhB8IivADQRF+ICjCDwRF+IGgCD8Q1LRG7my6tfkMzWrkLoFQfqu3dcQP20TWrSn8ZrZG0jZJLZL+\n3d23ptafoVk61y6uZZcAErp894TXrfptv5m1SLpF0qcknSVpnZmdVe3zAWisWj7zr5D0krvvc/cj\nku6StDaftgDUWy3hP1XSr8Y87s2W/R4z22Bm3WbWfVSHa9gdgDzVEv7xvlR41/nB7t7h7iV3L7Wq\nrYbdAchTLeHvlbRwzOMPSdpfWzsAGqWW8D8haamZLTaz6ZI+LWlXPm0BqLeqh/rc/ZiZbZT0Q40M\n9XW6+3O5dQagrmoa53f3ByU9mFMvABqIn/cCQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB\nEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivAD\nQRF+ICjCDwRF+IGgCD8QVE2z9JpZj6RDkoYkHXP3Uh5NIT82Lf2fuOUDc+u6/+f/elHZ2tDM4eS2\npy0ZSNZnftWS9f+7aXrZ2p7S3cltDwy9nayfe++mZP30v3o8WW8GNYU/88fufiCH5wHQQLztB4Kq\nNfwu6Udm9qSZbcijIQCNUevb/pXuvt/M5kl6yMx+4e6PjF0h+6OwQZJmaGaNuwOQl5qO/O6+P7sd\nkHS/pBXjrNPh7iV3L7WqrZbdAchR1eE3s1lmNnv0vqTVkp7NqzEA9VXL2/75ku43s9HnucPdf5BL\nVwDqrurwu/s+SZ/IsZcpq+XMpcm6t7Um6/sven+y/s555cek29+XHq9+9BPp8e4i/ddvZifr//Sd\nNcl619l3lK29fPSd5LZb+y9J1j/4qCfrkwFDfUBQhB8IivADQRF+ICjCDwRF+IGg8jirL7yhVZ9M\n1m/afkuy/tHW8qeeTmVHfShZ/7ubP5esT3s7Pdx2/r0by9Zmv3osuW3bgfRQ4MzurmR9MuDIDwRF\n+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc6fg7bn9yfrT/52YbL+0db+PNvJ1aa+85L1fW+lL/29fcn3\nytbeHE6P08//1/9O1utp8p+wWxlHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IytwbN6J5srX7uXZx\nw/bXLAavPj9ZP7gmfXntlqdPStZ/9tWbT7inUTce+MNk/YmL0uP4Q2+8maz7+eWv7t7z9eSmWrzu\nZ+kV8C5dvlsHfTA9d3mGIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFVxnN/MOiVdKmnA3Zdly9ol\n3S1pkaQeSVe6+68r7SzqOH8lLXP/IFkfen0wWX/5jvJj9c9d2JncdsU/fi1Zn3dLcefU48TlPc6/\nXdLxE6FfL2m3uy+VtDt7DGASqRh+d39E0vGHnrWSdmT3d0i6LOe+ANRZtZ/557t7nyRlt/PyawlA\nI9T9Gn5mtkHSBkmaoZn13h2ACar2yN9vZgskKbsdKLeiu3e4e8ndS61qq3J3APJWbfh3SVqf3V8v\n6YF82gHQKBXDb2Z3SnpM0sfMrNfMPi9pq6RLzOxFSZdkjwFMIhU/87v7ujIlBuxzMnTg9Zq2P3pw\netXbfvwzP0/WX7u1Jf0Ew0NV7xvF4hd+QFCEHwiK8ANBEX4gKMIPBEX4gaCYonsKOPO6F8rWrj47\nPSL7H6ftTtYvuuKaZH323Y8n62heHPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjG+aeA1DTZr3/l\nzOS2r+x6J1m//sadyfrfXHl5su4/fV/Z2sJvPJbcVg2cPj4ijvxAUIQfCIrwA0ERfiAowg8ERfiB\noAg/EFTFKbrzxBTdzWfwz89P1m+/4ZvJ+uJpM6re98d3bkzWl97Wl6wf29dT9b6nqryn6AYwBRF+\nICjCDwRF+IGgCD8QFOEHgiL8QFAVx/nNrFPSpZIG3H1ZtmyLpC9Kei1bbbO7P1hpZ4zzTz6+cnmy\nfvLW3mT9zo/8sOp9n/HjLyTrH/v78tcxkKShF/dVve/JKu9x/u2S1oyz/Nvuvjz7VzH4AJpLxfC7\n+yOSBhvQC4AGquUz/0Yze9rMOs1sTm4dAWiIasN/q6QlkpZL6pP0rXIrmtkGM+s2s+6jOlzl7gDk\nrarwu3u/uw+5+7Ck2yStSKzb4e4ldy+1qq3aPgHkrKrwm9mCMQ8vl/RsPu0AaJSKl+42szslrZI0\n18x6Jd0gaZWZLZfkknokfamOPQKoA87nR01a5s9L1vdfdXrZWtd125LbvqfCG9PPvLw6WX/zgteT\n9amI8/kBVET4gaAIPxAU4QeCIvxAUIQfCIqhPhTmnt70FN0zbXqy/hs/kqxf+rVryz/3/V3JbScr\nhvoAVET4gaAIPxAU4QeCIvxAUIQfCIrwA0FVPJ8fsQ1fkL509y+vSE/RvWx5T9lapXH8Sm4ePCdZ\nn/lAd03PP9Vx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnn+KstCxZf+Hr6bH221buSNYvnJE+\np74Wh/1osv744OL0Ewz35djN1MORHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2YLJe2UdIqk\nYUkd7r7NzNol3S1pkaQeSVe6+6/r12pc0xaflqz/8uoPlq1tuequ5LZ/dtKBqnrKw+b+UrL+8Lbz\nkvU5O9LX/UfaRI78xyRtcvczJZ0n6RozO0vS9ZJ2u/tSSbuzxwAmiYrhd/c+d9+T3T8kaa+kUyWt\nlTT6868dki6rV5MA8ndCn/nNbJGkcyR1SZrv7n3SyB8ISfPybg5A/Uw4/GZ2kqTvS7rW3Q+ewHYb\nzKzbzLqP6nA1PQKogwmF38xaNRL82939vmxxv5ktyOoLJA2Mt627d7h7yd1LrWrLo2cAOagYfjMz\nSd+VtNfdbxpT2iVpfXZ/vaQH8m8PQL1M5JTelZI+K+kZM3sqW7ZZ0lZJ95jZ5yW9IumK+rQ4+U1b\n9OFk/c0/WpCsX/UPP0jWv/z++5L1etrUlx6Oe+zfyg/ntW//n+S2c4YZyquniuF3959IKjff98X5\ntgOgUfiFHxAU4QeCIvxAUIQfCIrwA0ERfiAoLt09QdMWnFK2Ntg5K7ntVxY/nKyvm91fVU952Pjq\nBcn6nlvTU3TP/d6zyXr7IcbqmxVHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IKsw4/5E/SV8m+shf\nDibrm09/sGxt9XvfrqqnvPQPvVO2duGuTcltz/jbXyTr7W+kx+mHk1U0M478QFCEHwiK8ANBEX4g\nKMIPBEX4gaAIPxBUmHH+nsvSf+deOPveuu37ljeWJOvbHl6drNtQuSunjzjjxpfL1pb2dyW3HUpW\nMZVx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoMzd0yuYLZS0U9IpGjl9u8Pdt5nZFklflPRatupm\ndy9/0rukk63dzzVm9Qbqpct366APpn8YkpnIj3yOSdrk7nvMbLakJ83soaz2bXf/ZrWNAihOxfC7\ne5+kvuz+ITPbK+nUejcGoL5O6DO/mS2SdI6k0d+MbjSzp82s08zmlNlmg5l1m1n3UR2uqVkA+Zlw\n+M3sJEnfl3Stux+UdKukJZKWa+SdwbfG287dO9y95O6lVrXl0DKAPEwo/GbWqpHg3+7u90mSu/e7\n+5C7D0u6TdKK+rUJIG8Vw29mJum7kva6+01jli8Ys9rlktLTtQJoKhP5tn+lpM9KesbMnsqWbZa0\nzsyWS3JJPZK+VJcOAdTFRL7t/4mk8cYNk2P6AJobv/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAI\nPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVfHS3bnuzOw1Sf87ZtFcSQca1sCJadbemrUvid6qlWdv\np7n7ByayYkPD/66dm3W7e6mwBhKatbdm7Uuit2oV1Rtv+4GgCD8QVNHh7yh4/ynN2luz9iXRW7UK\n6a3Qz/wAilP0kR9AQQoJv5mtMbPnzewlM7u+iB7KMbMeM3vGzJ4ys+6Ce+k0swEze3bMsnYze8jM\nXsxux50mraDetpjZq9lr95SZ/WlBvS00sx+b2V4ze87M/iJbXuhrl+irkNet4W/7zaxF0guSLpHU\nK+kJSevc/ecNbaQMM+uRVHL3wseEzexCSW9J2unuy7Jl/yxp0N23Zn8457j7dU3S2xZJbxU9c3M2\nocyCsTNLS7pM0udU4GuX6OtKFfC6FXHkXyHpJXff5+5HJN0laW0BfTQ9d39E0uBxi9dK2pHd36GR\n/3karkxvTcHd+9x9T3b/kKTRmaULfe0SfRWiiPCfKulXYx73qrmm/HZJPzKzJ81sQ9HNjGN+Nm36\n6PTp8wru53gVZ25upONmlm6a166aGa/zVkT4x5v9p5mGHFa6+yclfUrSNdnbW0zMhGZubpRxZpZu\nCtXOeJ23IsLfK2nhmMcfkrS/gD7G5e77s9sBSfer+WYf7h+dJDW7HSi4n99pppmbx5tZWk3w2jXT\njNdFhP8JSUvNbLGZTZf0aUm7CujjXcxsVvZFjMxslqTVar7Zh3dJWp/dXy/pgQJ7+T3NMnNzuZml\nVfBr12wzXhfyI59sKONfJLVI6nT3bzS8iXGY2Uc0crSXRiYxvaPI3szsTkmrNHLWV7+kGyT9p6R7\nJH1Y0iuSrnD3hn/xVqa3VRp56/q7mZtHP2M3uLcLJD0q6RlJw9nizRr5fF3Ya5foa50KeN34hR8Q\nFL/wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1P8D6+E2hIAP97kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f451372fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_dataset.train_data[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = train_dataset.train_data\n",
    "# x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.train_data = x.view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=32)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = tc.nn.Linear(28*28,1)\n",
    "#         self.l1 = tc.nn.Linear()\n",
    "#         self.l1 = tc.nn.Linear()\n",
    "        self.sigmoid = tc.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.l1(x))\n",
    "    \n",
    "\n",
    "model = Model()\n",
    "criterion = tc.nn.BCELoss(size_average=False)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewook/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = Variable(inputs.view(inputs.size()[0], -1)), Variable(labels) # x.view(x.size[0], -1) flattens x matrix to one-dimensional vector.\n",
    "        inputs, labels = inputs.type(tc.FloatTensor), labels.type(tc.FloatTensor)\n",
    "        \n",
    "        labels_predict = model(inputs)\n",
    "        loss = criterion(labels_predict, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 12 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 13 to 25 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 26 to 38 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 39 to 51 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 52 to 64 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 65 to 77 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 78 to 90 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 91 to 103 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 104 to 116 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 117 to 129 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 130 to 142 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 143 to 155 \n",
       "    0     0     0     0     0     0     0     0     0     3    18    18    18\n",
       "\n",
       "Columns 156 to 168 \n",
       "  126   136   175    26   166   255   247   127     0     0     0     0     0\n",
       "\n",
       "Columns 169 to 181 \n",
       "    0     0     0     0     0     0     0    30    36    94   154   170   253\n",
       "\n",
       "Columns 182 to 194 \n",
       "  253   253   253   253   225   172   253   242   195    64     0     0     0\n",
       "\n",
       "Columns 195 to 207 \n",
       "    0     0     0     0     0     0     0     0    49   238   253   253   253\n",
       "\n",
       "Columns 208 to 220 \n",
       "  253   253   253   253   253   251    93    82    82    56    39     0     0\n",
       "\n",
       "Columns 221 to 233 \n",
       "    0     0     0     0     0     0     0     0     0     0    18   219   253\n",
       "\n",
       "Columns 234 to 246 \n",
       "  253   253   253   253   198   182   247   241     0     0     0     0     0\n",
       "\n",
       "Columns 247 to 259 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 260 to 272 \n",
       "   80   156   107   253   253   205    11     0    43   154     0     0     0\n",
       "\n",
       "Columns 273 to 285 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 286 to 298 \n",
       "    0     0     0    14     1   154   253    90     0     0     0     0     0\n",
       "\n",
       "Columns 299 to 311 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 312 to 324 \n",
       "    0     0     0     0     0     0     0   139   253   190     2     0     0\n",
       "\n",
       "Columns 325 to 337 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 338 to 350 \n",
       "    0     0     0     0     0     0     0     0     0    11   190   253    70\n",
       "\n",
       "Columns 351 to 363 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 364 to 376 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0    35\n",
       "\n",
       "Columns 377 to 389 \n",
       "  241   225   160   108     1     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 390 to 402 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 403 to 415 \n",
       "    0     0    81   240   253   253   119    25     0     0     0     0     0\n",
       "\n",
       "Columns 416 to 428 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 429 to 441 \n",
       "    0     0     0     0     0    45   186   253   253   150    27     0     0\n",
       "\n",
       "Columns 442 to 454 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 455 to 467 \n",
       "    0     0     0     0     0     0     0     0    16    93   252   253   187\n",
       "\n",
       "Columns 468 to 480 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 481 to 493 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0   249\n",
       "\n",
       "Columns 494 to 506 \n",
       "  253   249    64     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 507 to 519 \n",
       "    0     0     0     0     0     0     0     0     0     0     0    46   130\n",
       "\n",
       "Columns 520 to 532 \n",
       "  183   253   253   207     2     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 533 to 545 \n",
       "    0     0     0     0     0     0     0     0     0     0     0    39   148\n",
       "\n",
       "Columns 546 to 558 \n",
       "  229   253   253   253   250   182     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 559 to 571 \n",
       "    0     0     0     0     0     0     0     0     0     0     0    24   114\n",
       "\n",
       "Columns 572 to 584 \n",
       "  221   253   253   253   253   201    78     0     0     0     0     0     0\n",
       "\n",
       "Columns 585 to 597 \n",
       "    0     0     0     0     0     0     0     0     0     0     0    23    66\n",
       "\n",
       "Columns 598 to 610 \n",
       "  213   253   253   253   253   198    81     2     0     0     0     0     0\n",
       "\n",
       "Columns 611 to 623 \n",
       "    0     0     0     0     0     0     0     0     0     0     0    18   171\n",
       "\n",
       "Columns 624 to 636 \n",
       "  219   253   253   253   253   195    80     9     0     0     0     0     0\n",
       "\n",
       "Columns 637 to 649 \n",
       "    0     0     0     0     0     0     0     0     0     0     0    55   172\n",
       "\n",
       "Columns 650 to 662 \n",
       "  226   253   253   253   253   244   133    11     0     0     0     0     0\n",
       "\n",
       "Columns 663 to 675 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 676 to 688 \n",
       "  136   253   253   253   212   135   132    16     0     0     0     0     0\n",
       "\n",
       "Columns 689 to 701 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 702 to 714 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 715 to 727 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 728 to 740 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 741 to 753 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 754 to 766 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 767 to 779 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 780 to 783 \n",
       "    0     0     0     0\n",
       "[torch.ByteTensor of size 1x784]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Variable(train_dataset.train_data[0].view(28, -1))\n",
    "# inputs = train_dataset.train_data[0]\n",
    "# inputs = Variable(inputs.view(inputs.size()[0], -1))\n",
    "# x = x.type(tc.FloatTensor)\n",
    "# train_dataset.train_labels[0]\n",
    "# len(x)\n",
    "# print(model(inputs_one))\n",
    "for i in range(100):\n",
    "#     print(i)\n",
    "    inputs = Variable(train_dataset.train_data[i].view(-1,28*28))\n",
    "    inputs = inputs.type(tc.FloatTensor)\n",
    "    if model(inputs).data[0].numpy()[0] != 1:\n",
    "        print(model(inputs))\n",
    "# model(inputs).data[0].numpy()[0]\n",
    "# test[0]\n",
    "# print(train_dataset.train_data[0].view(28*28, -1))\n",
    "\n",
    "# train_dataset.train_data[0].size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBThB4Ii/EBQhB8IivADQRF+IKj/B41RG//9wYbJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45133289b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inputs_one.view(28,28).data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2.2 CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR100(root='./Data/CIFAR100_Data', train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = datasets.CIFAR100(root='./Data/CIFAR100_Data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, num_workers=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = tc.nn.Linear(32*32*3, 1)\n",
    "        self.sigmoid = tc.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criterion = tc.nn.BCELoss(size_average=False)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewook/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "Process Process-28:\n",
      "Process Process-27:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torchvision/datasets/cifar.py\", line 122, in __getitem__\n",
      "    img = self.transform(img)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-282-bac4ae144baa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#          Variable(inputs.view(inputs.size()[0], -1)), Variable(labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrebuild_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHALLENGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message = %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.6/site-packages/torchvision/datasets/cifar.py\", line 119, in __getitem__\n",
      "    img = Image.fromarray(img)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 75, in to_tensor\n",
      "    if isinstance(img, torch.ByteTensor):\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2380, in fromarray\n",
      "    return frombuffer(mode, size, obj, \"raw\", rawmode, 0, 1)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2333, in frombuffer\n",
      "    return frombytes(mode, size, data, decoder_name, args)\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2266, in frombytes\n",
      "    im.frombytes(data, decoder_name, args)\n",
      "  File \"/home/sewook/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 755, in frombytes\n",
      "    s = d.decode(data)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for batch_idx, (data, label) in enumerate(train_loader, 0):\n",
    "        data, label = Variable(data.view(data.size()[0], -1)), Variable(label)\n",
    "        data, label = data.type(tc.FloatTensor), label.type(tc.FloatTensor)\n",
    "#          Variable(inputs.view(inputs.size()[0], -1)), Variable(labels)\n",
    "        \n",
    "        label_pred = model(data)\n",
    "        loss = criterion(label_pred, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
