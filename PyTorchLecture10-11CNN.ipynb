{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optional caption text](Figures/CNNArchitecture.jpg)\n",
    "(http://parse.ele.tue.nl/cluster/2/CNNArchitecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Convolution\n",
    "\n",
    "Convolve the filter with the image (i.e., slide over the image spatially, computing dot products).\n",
    "\n",
    "We are going to look at only small portion of image at once; In other words, we look at **patch** via **filter (also called \"kernel\" or \"feature detector\")**.\n",
    "Each element in a filter is a weight. You can consider filter as a **weight vector for a patch.**\n",
    "\n",
    "We move filter at each time, and the length of the distance we move the filter is called **stride.**\n",
    "\n",
    "We can add **padding** by adding certain constant to the boundaries of images. It is often called **zero-padding** because we use zero as the value for the padding.\n",
    "\n",
    "As a result of application of filter to an image, you get a **feature map (or activation map).**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "After applying convolutional layer, apply **pooling layer** to reduce the amount of information. There are mainly two kinds of poolings:\n",
    "1. Max pooling\n",
    "2. Average pooling.\n",
    "\n",
    "### The advantage of using convolutional neural net:\n",
    "1. the size of weight matrix is smaller than fully connected net\n",
    "2. more flexible to handle images.\n",
    "![optional](Figures/cnn1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # You can also do\n",
    "        # super().__init__()\n",
    "        self.conv1 = tc.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = tc.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = tc.nn.MaxPool2d(2)\n",
    "        \n",
    "        self.relu = tc.nn.functional.relu\n",
    "        self.fc = tc.nn.Linear(320,10)\n",
    "        # self.fc = tc.nn.Linear(??,10)\n",
    "        # In order to determine what should go in ??, you can either use\n",
    "        ## 1. Error: If you put ??, you'll run into a RuntimeError:size mismatch, m1:[64x320], m2:[100x10]\n",
    "        ## and \"320\" is the number that should go into ??.\n",
    "        ## 2. x.size: You can print(x.size) and check the size of x.\n",
    "        self.log_softmax = tc.nn.functional.log_softmax\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(self.mp(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(self.mp(x))\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return self.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size() # [0]: Batch size, 1: Number of channels, 28*28: Input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=True,\n",
    "#                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                             transforms.Normalize((0.13066062, ), (0.30810776, ))]))\n",
    "test_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=False,\n",
    "                              transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                             transforms.Normalize((0.13066062, ), (0.30810776, ))]))\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle = True, batch_size=64, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# criterion = tc.nn.BCELoss()\n",
    "# optimizer = tc.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model = Net()\n",
    "criterion = tc.nn.CrossEntropyLoss()\n",
    "# CHOICE OF CRITERION IS VERY IMPORTANT!!!\n",
    "# criterion = tc.nn.BCELoss() does not work.\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        pred_label = model(data)\n",
    "        loss = criterion(pred_label, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                 .format(epoch, batch_idx*len(data), len(train_loader.dataset), 100.*batch_idx/len(train_loader),\n",
    "                 loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, label in test_loader:\n",
    "        data, label = Variable(data, volatile=True), Variable(label)\n",
    "        pred_label = model(data)\n",
    "        test_loss += criterion(pred_label, label).data[0]\n",
    "#         test_loss += tc.nn.functional.nll_loss(pred_label, label).data[0]\n",
    "                \n",
    "        pred = pred_label.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewook/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.330516\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 2.220621\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 2.125867\n",
      "Train Epoch: 0 [1920/60000 (3%)]\tLoss: 1.935391\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 1.508371\n",
      "Train Epoch: 0 [3200/60000 (5%)]\tLoss: 1.131037\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 0.966733\n",
      "Train Epoch: 0 [4480/60000 (7%)]\tLoss: 0.725739\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 0.490665\n",
      "Train Epoch: 0 [5760/60000 (10%)]\tLoss: 0.472078\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.480888\n",
      "Train Epoch: 0 [7040/60000 (12%)]\tLoss: 0.695810\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 0.377113\n",
      "Train Epoch: 0 [8320/60000 (14%)]\tLoss: 0.449468\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 0.488793\n",
      "Train Epoch: 0 [9600/60000 (16%)]\tLoss: 0.390571\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 0.210641\n",
      "Train Epoch: 0 [10880/60000 (18%)]\tLoss: 0.712056\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 0.358344\n",
      "Train Epoch: 0 [12160/60000 (20%)]\tLoss: 0.289750\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.326937\n",
      "Train Epoch: 0 [13440/60000 (22%)]\tLoss: 0.555935\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 0.430634\n",
      "Train Epoch: 0 [14720/60000 (25%)]\tLoss: 0.147671\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 0.204642\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.141355\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 0.325184\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 0.253296\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 0.231412\n",
      "Train Epoch: 0 [18560/60000 (31%)]\tLoss: 0.286670\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.295122\n",
      "Train Epoch: 0 [19840/60000 (33%)]\tLoss: 0.189701\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.236796\n",
      "Train Epoch: 0 [21120/60000 (35%)]\tLoss: 0.647015\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 0.180948\n",
      "Train Epoch: 0 [22400/60000 (37%)]\tLoss: 0.343793\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.453536\n",
      "Train Epoch: 0 [23680/60000 (39%)]\tLoss: 0.340335\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.192470\n",
      "Train Epoch: 0 [24960/60000 (42%)]\tLoss: 0.196851\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.082222\n",
      "Train Epoch: 0 [26240/60000 (44%)]\tLoss: 0.045997\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.164024\n",
      "Train Epoch: 0 [27520/60000 (46%)]\tLoss: 0.340875\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 0.159554\n",
      "Train Epoch: 0 [28800/60000 (48%)]\tLoss: 0.252323\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.092295\n",
      "Train Epoch: 0 [30080/60000 (50%)]\tLoss: 0.170384\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.247158\n",
      "Train Epoch: 0 [31360/60000 (52%)]\tLoss: 0.321345\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.201418\n",
      "Train Epoch: 0 [32640/60000 (54%)]\tLoss: 0.112719\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 0.334306\n",
      "Train Epoch: 0 [33920/60000 (57%)]\tLoss: 0.149801\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.326623\n",
      "Train Epoch: 0 [35200/60000 (59%)]\tLoss: 0.214235\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 0.234643\n",
      "Train Epoch: 0 [36480/60000 (61%)]\tLoss: 0.080448\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.247265\n",
      "Train Epoch: 0 [37760/60000 (63%)]\tLoss: 0.066823\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.098367\n",
      "Train Epoch: 0 [39040/60000 (65%)]\tLoss: 0.158219\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.132204\n",
      "Train Epoch: 0 [40320/60000 (67%)]\tLoss: 0.118493\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 0.101362\n",
      "Train Epoch: 0 [41600/60000 (69%)]\tLoss: 0.198166\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 0.240024\n",
      "Train Epoch: 0 [42880/60000 (71%)]\tLoss: 0.171213\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 0.130128\n",
      "Train Epoch: 0 [44160/60000 (74%)]\tLoss: 0.210252\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.108306\n",
      "Train Epoch: 0 [45440/60000 (76%)]\tLoss: 0.060409\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 0.117412\n",
      "Train Epoch: 0 [46720/60000 (78%)]\tLoss: 0.036164\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 0.096766\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.218328\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.076576\n",
      "Train Epoch: 0 [49280/60000 (82%)]\tLoss: 0.134456\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.211797\n",
      "Train Epoch: 0 [50560/60000 (84%)]\tLoss: 0.091712\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.129597\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.060603\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.209009\n",
      "Train Epoch: 0 [53120/60000 (88%)]\tLoss: 0.165393\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.214543\n",
      "Train Epoch: 0 [54400/60000 (91%)]\tLoss: 0.109651\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.122205\n",
      "Train Epoch: 0 [55680/60000 (93%)]\tLoss: 0.112245\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.102608\n",
      "Train Epoch: 0 [56960/60000 (95%)]\tLoss: 0.125710\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.044759\n",
      "Train Epoch: 0 [58240/60000 (97%)]\tLoss: 0.116184\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.280991\n",
      "Train Epoch: 0 [59520/60000 (99%)]\tLoss: 0.114057\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 9664/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.113089\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.039513\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.168313\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.166163\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.120594\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.046731\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.240828\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.099617\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.062538\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.026826\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.041743\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.093545\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.131412\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.024958\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.220002\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.125629\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.159327\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.110397\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.217604\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.136716\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.019392\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.231040\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.112018\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.114081\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.135823\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.058125\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.035540\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.022483\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.158872\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.170298\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.055875\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.144091\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.192783\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.045763\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.158195\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.057207\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.040953\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.068307\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.083104\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.104187\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.295089\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.116631\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.123524\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.120819\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.101373\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.103996\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.115339\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.056467\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.024559\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.110278\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.032311\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.213775\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.118992\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.072614\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.122054\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.036157\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.097201\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.084327\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.043545\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.049369\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.191377\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.124999\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.052921\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.130730\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.100848\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.026396\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.063356\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.069685\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.021367\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.031872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.141925\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.054255\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.046486\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.115104\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.053022\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.309232\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.085471\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.113151\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.088760\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.022616\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.038994\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.131400\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.140671\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.073617\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.107248\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.085924\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.042484\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.085894\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.045710\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.018368\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.083764\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.057814\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.119067\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.050395\n",
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 9734/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Advanced CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Inception modules\n",
    "\n",
    "When not know which size of filter to choose, **use all different filters at the same time.**\n",
    "\n",
    "You use $1\\times1$ filter because **it reduces the number of operations to a huge extent.**\n",
    "\n",
    "<img align=\"left\" src=\"./Figures/noreduce.png\" width=450>\n",
    "<img align=\"left\" src=\"./Figures/reduced1.png\"width=450>\n",
    "\n",
    "<!-- ![optional](./Figures/noreduce.png)![optional](./Figures/reduced1.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make an inception module\n",
    "<img align=\"center\" src=\"./Figures/inception_module.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(tc.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        \n",
    "        self.branch_pool_1 = tc.nn.Conv2d(in_channels=in_channels, out_channels=24, kernel_size=1)\n",
    "        \n",
    "        self.branch1x1 = tc.nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        \n",
    "        self.branch5x5_1 = tc.nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        self.branch5x5_2 = tc.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.branch3x3_1 = tc.nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        self.branch3x3_2 = tc.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=3, padding=1)\n",
    "        self.branch3x3_3 = tc.nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, padding=1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch_pool = self.branch_pool_1(tc.nn.functional.avg_pool2d(x, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch5x5 = self.branch5x5_2(self.branch5x5_1(x))\n",
    "        branch3x3 = self.branch3x3_3(self.branch3x3_2(self.branch3x3_1(x)))\n",
    "        \n",
    "        outputs = [branch_pool, branch1x1, branch5x5, branch3x3]\n",
    "        return tc.cat(outputs, 1) # Concatenate output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = tc.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = tc.nn.Conv2d(88, 20, kernel_size=5)\n",
    "        \n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "        \n",
    "        self.mp = tc.nn.MaxPool2d(2)\n",
    "        self.fc = tc.nn.Linear(1408, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = tc.nn.functional.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = tc.nn.functional.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return tc.nn.functional.log_softmax(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=True,\n",
    "           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.13066062, ), (0.30810776, ))]), download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./Data/MNIST_Data/\", train=False,\n",
    "                              transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.13066062, ), (0.30810776, ))]))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=64, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=64, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = tc.nn.CrossEntropyLoss()\n",
    "# CHOICE OF CRITERION IS VERY IMPORTANT!!!\n",
    "# criterion = tc.nn.BCELoss() does not work.\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        pred_label = model(data)\n",
    "        loss = criterion(pred_label, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                 .format(epoch, batch_idx*len(data), len(train_loader.dataset), 100.*batch_idx/len(train_loader),\n",
    "                 loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    for data, label in test_loader:\n",
    "        data, label = Variable(data, volatile=True), Variable(label)\n",
    "        pred_label = model(data)\n",
    "        test_loss += criterion(pred_label, label).data[0]\n",
    "        \n",
    "        pred = pred_label.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewook/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.300869\n",
      "Train Epoch: 0 [640/60000 (1%)]\tLoss: 2.283628\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 2.245745\n",
      "Train Epoch: 0 [1920/60000 (3%)]\tLoss: 2.124050\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 1.925311\n",
      "Train Epoch: 0 [3200/60000 (5%)]\tLoss: 1.340260\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 0.958326\n",
      "Train Epoch: 0 [4480/60000 (7%)]\tLoss: 0.887804\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 0.835170\n",
      "Train Epoch: 0 [5760/60000 (10%)]\tLoss: 0.682686\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.718650\n",
      "Train Epoch: 0 [7040/60000 (12%)]\tLoss: 0.572362\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 0.427018\n",
      "Train Epoch: 0 [8320/60000 (14%)]\tLoss: 0.258705\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 0.395186\n",
      "Train Epoch: 0 [9600/60000 (16%)]\tLoss: 0.494061\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 0.285434\n",
      "Train Epoch: 0 [10880/60000 (18%)]\tLoss: 0.512879\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 0.390059\n",
      "Train Epoch: 0 [12160/60000 (20%)]\tLoss: 0.452193\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.201620\n",
      "Train Epoch: 0 [13440/60000 (22%)]\tLoss: 0.313794\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 0.174458\n",
      "Train Epoch: 0 [14720/60000 (25%)]\tLoss: 0.148357\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 0.291249\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.447433\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 0.194552\n",
      "Train Epoch: 0 [17280/60000 (29%)]\tLoss: 0.366510\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 0.175303\n",
      "Train Epoch: 0 [18560/60000 (31%)]\tLoss: 0.324505\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.162262\n",
      "Train Epoch: 0 [19840/60000 (33%)]\tLoss: 0.336543\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.491500\n",
      "Train Epoch: 0 [21120/60000 (35%)]\tLoss: 0.144870\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 0.422905\n",
      "Train Epoch: 0 [22400/60000 (37%)]\tLoss: 0.322323\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.214571\n",
      "Train Epoch: 0 [23680/60000 (39%)]\tLoss: 0.330314\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.143898\n",
      "Train Epoch: 0 [24960/60000 (42%)]\tLoss: 0.308554\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.186539\n",
      "Train Epoch: 0 [26240/60000 (44%)]\tLoss: 0.251685\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.157359\n",
      "Train Epoch: 0 [27520/60000 (46%)]\tLoss: 0.124765\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 0.113387\n",
      "Train Epoch: 0 [28800/60000 (48%)]\tLoss: 0.177549\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.187748\n",
      "Train Epoch: 0 [30080/60000 (50%)]\tLoss: 0.096097\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.155776\n",
      "Train Epoch: 0 [31360/60000 (52%)]\tLoss: 0.149611\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.318327\n",
      "Train Epoch: 0 [32640/60000 (54%)]\tLoss: 0.201166\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 0.170267\n",
      "Train Epoch: 0 [33920/60000 (57%)]\tLoss: 0.185760\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.257280\n",
      "Train Epoch: 0 [35200/60000 (59%)]\tLoss: 0.107155\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 0.088193\n",
      "Train Epoch: 0 [36480/60000 (61%)]\tLoss: 0.216376\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.146159\n",
      "Train Epoch: 0 [37760/60000 (63%)]\tLoss: 0.166685\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.162395\n",
      "Train Epoch: 0 [39040/60000 (65%)]\tLoss: 0.154143\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.248541\n",
      "Train Epoch: 0 [40320/60000 (67%)]\tLoss: 0.084475\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 0.233064\n",
      "Train Epoch: 0 [41600/60000 (69%)]\tLoss: 0.361561\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 0.146689\n",
      "Train Epoch: 0 [42880/60000 (71%)]\tLoss: 0.129316\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 0.222506\n",
      "Train Epoch: 0 [44160/60000 (74%)]\tLoss: 0.074758\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.156807\n",
      "Train Epoch: 0 [45440/60000 (76%)]\tLoss: 0.159705\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 0.180148\n",
      "Train Epoch: 0 [46720/60000 (78%)]\tLoss: 0.061838\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 0.083144\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.110664\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.101980\n",
      "Train Epoch: 0 [49280/60000 (82%)]\tLoss: 0.153353\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.128074\n",
      "Train Epoch: 0 [50560/60000 (84%)]\tLoss: 0.050745\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.191336\n",
      "Train Epoch: 0 [51840/60000 (86%)]\tLoss: 0.087336\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.064640\n",
      "Train Epoch: 0 [53120/60000 (88%)]\tLoss: 0.174088\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.066739\n",
      "Train Epoch: 0 [54400/60000 (91%)]\tLoss: 0.099686\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.109807\n",
      "Train Epoch: 0 [55680/60000 (93%)]\tLoss: 0.173538\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.054096\n",
      "Train Epoch: 0 [56960/60000 (95%)]\tLoss: 0.069528\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.117908\n",
      "Train Epoch: 0 [58240/60000 (97%)]\tLoss: 0.056182\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.050515\n",
      "Train Epoch: 0 [59520/60000 (99%)]\tLoss: 0.158401\n",
      "\n",
      "Test set: Average loss: 0.0018, Accuracy: 9641/10000 (96%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.171052\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.053356\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.120613\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.199848\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.137870\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.077120\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.139682\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.072124\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.182633\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.026224\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.103632\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.076487\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.141338\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.314468\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.280329\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.101985\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.039651\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.177834\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.194713\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.117572\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.101543\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.078041\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.130272\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.088744\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.053499\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.039166\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.156103\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.221614\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.126998\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.046690\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.284115\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.077350\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.192230\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.191560\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.126228\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.222482\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.201804\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.053162\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.193352\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.149530\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.261008\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.179449\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.093692\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.017975\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.196249\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.073904\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.129165\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.049021\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.083469\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.056018\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.086396\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.110623\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.042430\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.018242\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.177828\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.066626\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.048712\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.048345\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.074171\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.084704\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.101303\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.195089\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.154230\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.165475\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.135999\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.016500\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.094491\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.084929\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.139956\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.023184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.102901\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.049198\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.072737\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.092172\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.086326\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.051010\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.096345\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.043783\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.070320\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.108540\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.075952\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.199443\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.028848\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.020512\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.051119\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.076151\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.071227\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.038767\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.118135\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.057401\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.060208\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.037607\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.008533\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.048143\n",
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 9739/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
