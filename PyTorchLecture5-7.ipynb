{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Linear regression in the PyTorch way\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Rhythm:\n",
    "    1. Design your model using `class` with `Variable`\n",
    "    2. Construct loss and optimizer\n",
    "    3. Training cycle\n",
    "        (forward, backward, update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Design your model using `class` with `Variable`\n",
    "\n",
    "To design a **linear regression model** with PyTorch, we'll use a predefined function called **`tc.nn.Linear`.** `tc.nn.Linear` receives a Variable, and makes a prediction with a random weight. For example, we can construct a linear regression model with **one input and one output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to False, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
      "          additional dimensions\n",
      "        - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
      "          are the same shape as the input.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            (out_features x in_features)\n",
      "        bias:   the learnable bias of the module of shape (out_features)\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = autograd.Variable(torch.randn(128, 20))\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(tc.nn.Linear.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.1464\n",
       " 2.0271\n",
       " 2.9079\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.nn.Linear(1,1)(Variable(tc.Tensor([[1.0], [2.0], [3.0]]))) # (1,1): one input and one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear(in_features=1, out_features=1)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.nn.Linear.__repr__(tc.nn.Linear(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the (weight) parameter of `tc.nn.Linear` by using `.parameters()` (which is from a class `nn.Parameter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Parameter(Variable):\n",
      "    r\"\"\"A kind of Variable that is to be considered a module parameter.\n",
      "\n",
      "    Parameters are :class:`~torch.autograd.Variable` subclasses, that have a\n",
      "    very special property when used with :class:`Module` s - when they're\n",
      "    assigned as Module attributes they are automatically added to the list of\n",
      "    its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
      "    Assigning a Variable doesn't have such effect. This is because one might\n",
      "    want to cache some temporary state, like last hidden state of the RNN, in\n",
      "    the model. If there was no such class as :class:`Parameter`, these\n",
      "    temporaries would get registered too.\n",
      "\n",
      "    Another difference is that parameters can't be volatile and that they\n",
      "    require gradient by default.\n",
      "\n",
      "    Arguments:\n",
      "        data (Tensor): parameter tensor.\n",
      "        requires_grad (bool, optional): if the parameter requires gradient. See\n",
      "            :ref:`excluding-subgraphs` for more details.\n",
      "    \"\"\"\n",
      "    def __new__(cls, data=None, requires_grad=True):\n",
      "        return super(Parameter, cls).__new__(cls, data, requires_grad=requires_grad)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return 'Parameter containing:' + self.data.__repr__()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(tc.nn.Parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.5686\n",
       " [torch.FloatTensor of size 1x1], Parameter containing:\n",
       "  0.8710\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tc.nn.Linear(1,1).parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when you make a model, you'll make the model as a **subclass of `tc.nn.Module` to use all the functions therein.** Therefore, you need to put `tc.nn.Module` as an object of Model class by using `super`:\n",
    "\n",
    "`Model(tc.nn.Module):\n",
    "    def __init(self):\n",
    "        super(Model, self).__init__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from  torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(tc.Tensor([[1.0], [2.0], [3.0]])) # Define 3X1 dataset (matrix).\n",
    "y_data = Variable(tc.Tensor([[2.0], [4.0], [6.0]])) # Define 3X1 dataset (matrix).\n",
    "\n",
    "class Model(tc.nn.Module): # create class Model, which is a subclass of torch.nn.Module\n",
    "    def __init__(self): #Initialize class: Codes to be executed when the object is created\n",
    "        super(Model, self).__init__() #super lets the class call the parent classes (torch.nn.Module in this case) before all the codes below are executed.\n",
    "        # You can do some initalization task, or create some element and components for neural net.\n",
    "        self.linear = tc.nn.Linear(1, 1) # Linear model block: Input_size (1), Output_size (1)\n",
    "        \n",
    "    def forward(self, x): # one input (x)\n",
    "        # We don't use our own weight `w`, but we use the block we initialized in line 11.\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "            \n",
    "#Once created a class, you can create an instance using the class\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1107\n",
       " 0.2255\n",
       " 0.3403\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Construct loss (or select loss from the api) and optimizer\n",
    "\n",
    "Now we will calculate loss and do optimization. In order to to optimization, **you need to be able to use model parameter, which you can get by using `model.parameters()` (which is from a class `nn.Parameter`).**\n",
    "\n",
    "`nn.Parameter` class is a very very simple subclass of `Variable`, but it has a special behaviour when assigned as a module attribute - it gets added to the parameter list, and is returned when you call model.parameters(). Variables aren’t added automatically, we thought of many cases where you’d want to cache some Variable in your model, without adding them to parameters (e.g. when writing an recurrent network you might want to save the last hidden state so you don’t have to pass it over and over).\n",
    "\n",
    "Also Parameters are convenient, because **they `require_grad` by default.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.1892  0.1998  0.5011\n",
       " [torch.FloatTensor of size 1x3], Parameter containing:\n",
       " -0.5761\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tc.nn.Linear(3,1).parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  -9.5706\n",
       " [torch.FloatTensor of size 1x1], Parameter containing:\n",
       " -0.8476\n",
       " [torch.FloatTensor of size 1]]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First element `list(model.parameters())[0]` corresponds to the input (and the size is the same as the input size), and the second element `list(model.parameters())[1]` corresponds to the output (and the size is the same as the output size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use MSE Loss\n",
    "criterion = tc.nn.MSELoss(size_average = False)\n",
    "                         \n",
    "# We use SGD. We need to pass what parameter / variable we are updating\n",
    "# In this case, we update model variable using model.parameters()\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Training cycle: Forward, loss, backward, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.049306869506836\n",
      "1 4.050587177276611\n",
      "2 1.8249815702438354\n",
      "3 0.8338918685913086\n",
      "4 0.39237838983535767\n",
      "5 0.19552509486675262\n",
      "6 0.10759192705154419\n",
      "7 0.06815139949321747\n",
      "8 0.05030248314142227\n",
      "9 0.04206973686814308\n",
      "10 0.03812200203537941\n",
      "11 0.0360858254134655\n",
      "12 0.03490469977259636\n",
      "13 0.03410810977220535\n",
      "14 0.033486586064100266\n",
      "15 0.032946910709142685\n",
      "16 0.03244740515947342\n",
      "17 0.03196948766708374\n",
      "18 0.03150486573576927\n",
      "19 0.03104979544878006\n",
      "20 0.03060251660645008\n",
      "21 0.030162280425429344\n",
      "22 0.02972857840359211\n",
      "23 0.029301246628165245\n",
      "24 0.02888011746108532\n",
      "25 0.028465040028095245\n",
      "26 0.028055943548679352\n",
      "27 0.027652747929096222\n",
      "28 0.027255330234766006\n",
      "29 0.026863649487495422\n",
      "30 0.026477541774511337\n",
      "31 0.02609703503549099\n",
      "32 0.0257219597697258\n",
      "33 0.025352314114570618\n",
      "34 0.024987956508994102\n",
      "35 0.02462884411215782\n",
      "36 0.024274900555610657\n",
      "37 0.023926017805933952\n",
      "38 0.023582151159644127\n",
      "39 0.023243235424160957\n",
      "40 0.022909199818968773\n",
      "41 0.022579971700906754\n",
      "42 0.0222554299980402\n",
      "43 0.021935606375336647\n",
      "44 0.021620357409119606\n",
      "45 0.021309612318873405\n",
      "46 0.021003365516662598\n",
      "47 0.020701516419649124\n",
      "48 0.02040400169789791\n",
      "49 0.02011076547205448\n",
      "50 0.019821763038635254\n",
      "51 0.019536901265382767\n",
      "52 0.01925608702003956\n",
      "53 0.018979357555508614\n",
      "54 0.01870660111308098\n",
      "55 0.018437759950757027\n",
      "56 0.018172793090343475\n",
      "57 0.017911607399582863\n",
      "58 0.017654210329055786\n",
      "59 0.017400486394762993\n",
      "60 0.017150426283478737\n",
      "61 0.01690395176410675\n",
      "62 0.016660982742905617\n",
      "63 0.01642155833542347\n",
      "64 0.01618555746972561\n",
      "65 0.015952961519360542\n",
      "66 0.01572365313768387\n",
      "67 0.015497715212404728\n",
      "68 0.01527497824281454\n",
      "69 0.01505542453378439\n",
      "70 0.014839059673249722\n",
      "71 0.014625806361436844\n",
      "72 0.014415601268410683\n",
      "73 0.014208439737558365\n",
      "74 0.014004262164235115\n",
      "75 0.013802996836602688\n",
      "76 0.013604617677628994\n",
      "77 0.013409093022346497\n",
      "78 0.013216395862400532\n",
      "79 0.013026420027017593\n",
      "80 0.012839224189519882\n",
      "81 0.012654723599553108\n",
      "82 0.01247282326221466\n",
      "83 0.012293585576117039\n",
      "84 0.012116890400648117\n",
      "85 0.011942753568291664\n",
      "86 0.011771139688789845\n",
      "87 0.01160196028649807\n",
      "88 0.011435218155384064\n",
      "89 0.011270883493125439\n",
      "90 0.011108892038464546\n",
      "91 0.010949247516691685\n",
      "92 0.010791880078613758\n",
      "93 0.010636807419359684\n",
      "94 0.010483925230801105\n",
      "95 0.010333254933357239\n",
      "96 0.010184739716351032\n",
      "97 0.010038360953330994\n",
      "98 0.009894099086523056\n",
      "99 0.009751907549798489\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    # Predict Y by passing every elements of x (input whole x matrix)\n",
    "    y_pred = model.forward(x_data)\n",
    "#     print(x_data)\n",
    "#     print(y_pred)\n",
    "    \n",
    "    #Compute loss: compare between predictive value and real value\n",
    "    loss = criterion(y_pred, x_data)\n",
    "    print(epoch, loss.data[0])\n",
    "    \n",
    "    #Backward pass\n",
    "    #3.1. Before we compute gradients, we initialize all gradients.\n",
    "    optimizer.zero_grad()\n",
    "    #3.2 Compute all gradients with respect to ALL VARIABLES\n",
    "    loss.backward()\n",
    "    #3.3 UPDATE variable. What variables? model.parameters().\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.0004\n",
      " 4.0001\n",
      " 5.9998\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = tc.nn.Linear(1,1)\n",
    "        \n",
    "    # YOU NEED TO NAME IT AS FORWARD.\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "x_data = Variable(tc.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(tc.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "criterion = tc.nn.MSELoss(size_average=False)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr=0.01)\n",
    "# Best option: optimizer = tc.optim.Rprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "print(model.forward(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.0004\n",
       " 4.0001\n",
       " 5.9998\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Logistic regression\n",
    "\n",
    "for **Binary prediction.** (Logistic regression is a **classification algorithm.**\n",
    "\n",
    "How to turn our model to predict 0 & 1 instead of real numbers? You can use **sigmoid function.** Sigmoid function **squashes real numbers between 0 and 1.**\n",
    "\n",
    "Therefore, if you want to turn real-valued function to binary (0 and 1), you can wrap the real-valued function with sigmoid:\n",
    "\n",
    "<center>$\\hat{y} = \\sigma(xw + b)$</center>\n",
    "where\n",
    "<center>$\\sigma(x)=\\frac{1}{e^{-x} + 1}$</center>\n",
    "\n",
    "Also, you need to use **cross entropy loss** instead of maximum square estimate (MSE), because MSE does not work well for sigmoid function.\n",
    "\n",
    "<center>$\\text{loss} = -\\frac{1}{N}\\sum_{n=1}^{N} y_n \\log(\\hat{y_n}) + (1-y_n)\\log(1-\\hat{y_n}) $</center>\n",
    "\n",
    "For binary-decision making, you set a threshold (cutoff value) that determines binary decision. The output of sigmoid function is between 0 and 1, and **it is common in the binary decision making that you consider the decision to be 1 if the output of sigmoid is larger than 0.5.** You can use different threshold, but then you need to be careful to decide the right threshold. About choosing the correct threshold, see\n",
    "\n",
    "1. <http://ethen8181.github.io/machine-learning/unbalanced/unbalanced.html>\n",
    "2. [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "x_data = Variable(tc.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(tc.Tensor([[0.0], [0.0], [1.0], [1.0]]))\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = tc.nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # wrap linear output with sigmoid function.\n",
    "        y_pred = tc.nn.functional.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = Model()\n",
    "# Binary cross-entropy loss\n",
    "criterion = tc.nn.BCELoss(size_average=True)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4705265462398529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8106\n",
       "[torch.FloatTensor of size 1x1]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loss.data[0])\n",
    "model(Variable(tc.Tensor([[4.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use different activation functions: `ReLU`, `SELU`, `Threshold`, `Tanh` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9745\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "x_data = Variable(tc.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(tc.Tensor([[0.0], [0.0], [1.0], [1.0]]))\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = tc.nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return tc.nn.functional.sigmoid(self.linear(x))\n",
    "        # Error if I try tanh or relu. why?\n",
    "    \n",
    "    \n",
    "model = Model()\n",
    "criterion = tc.nn.BCELoss(size_average=True)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(model(Variable(tc.Tensor([[7.0]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Go wide and deep\n",
    "\n",
    "## 1.5.1 Go Wide\n",
    "\n",
    "If we have more than one feature, we use **matrix multiplication** for our linear regression. For example, if we have two inputs and one output, we should use\n",
    "\n",
    "`linear = torch.nn.Linear(2,1)\n",
    "\n",
    "y_pred = linear(x_data)`\n",
    "\n",
    "Here, `x_data` has more than one column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.2 Go Deep\n",
    "\n",
    "We can use **multiple layers;** we can create multiple linear components, and connect them together.\n",
    "\n",
    "The problem of using sigmoid function is that it can incur **vanishing gradient problem.**\n",
    "\n",
    "How to solve? **Use other types of activation function (ReLU works really well)**\n",
    "\n",
    "![optional caption text](Figures/activation-functions.png)\n",
    "<https://rasbt.github.io/mlxtend/user_guide/general_concepts/activation-functions/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualizing activation functions, see <https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6496\n",
      " 0.6484\n",
      " 0.6493\n",
      " 0.6487\n",
      " 0.6493\n",
      " 0.6490\n",
      " 0.6482\n",
      " 0.6494\n",
      " 0.6492\n",
      " 0.6502\n",
      " 0.6492\n",
      " 0.6498\n",
      " 0.6504\n",
      " 0.6492\n",
      " 0.6491\n",
      " 0.6497\n",
      " 0.6492\n",
      " 0.6478\n",
      " 0.6485\n",
      " 0.6490\n",
      " 0.6499\n",
      " 0.6499\n",
      " 0.6496\n",
      " 0.6497\n",
      " 0.6496\n",
      " 0.6482\n",
      " 0.6505\n",
      " 0.6495\n",
      " 0.6497\n",
      " 0.6489\n",
      " 0.6483\n",
      " 0.6493\n",
      " 0.6500\n",
      " 0.6487\n",
      " 0.6499\n",
      " 0.6499\n",
      " 0.6486\n",
      " 0.6495\n",
      " 0.6485\n",
      " 0.6497\n",
      " 0.6499\n",
      " 0.6507\n",
      " 0.6493\n",
      " 0.6496\n",
      " 0.6483\n",
      " 0.6485\n",
      " 0.6492\n",
      " 0.6489\n",
      " 0.6484\n",
      " 0.6480\n",
      " 0.6487\n",
      " 0.6502\n",
      " 0.6495\n",
      " 0.6485\n",
      " 0.6494\n",
      " 0.6497\n",
      " 0.6502\n",
      " 0.6490\n",
      " 0.6488\n",
      " 0.6495\n",
      " 0.6489\n",
      " 0.6483\n",
      " 0.6493\n",
      " 0.6491\n",
      " 0.6500\n",
      " 0.6496\n",
      " 0.6482\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6487\n",
      " 0.6506\n",
      " 0.6490\n",
      " 0.6485\n",
      " 0.6480\n",
      " 0.6495\n",
      " 0.6490\n",
      " 0.6492\n",
      " 0.6485\n",
      " 0.6482\n",
      " 0.6482\n",
      " 0.6494\n",
      " 0.6491\n",
      " 0.6498\n",
      " 0.6486\n",
      " 0.6502\n",
      " 0.6484\n",
      " 0.6503\n",
      " 0.6484\n",
      " 0.6485\n",
      " 0.6491\n",
      " 0.6494\n",
      " 0.6494\n",
      " 0.6492\n",
      " 0.6493\n",
      " 0.6484\n",
      " 0.6478\n",
      " 0.6485\n",
      " 0.6489\n",
      " 0.6488\n",
      " 0.6482\n",
      " 0.6501\n",
      " 0.6483\n",
      " 0.6486\n",
      " 0.6486\n",
      " 0.6492\n",
      " 0.6487\n",
      " 0.6483\n",
      " 0.6494\n",
      " 0.6486\n",
      " 0.6497\n",
      " 0.6483\n",
      " 0.6486\n",
      " 0.6491\n",
      " 0.6498\n",
      " 0.6492\n",
      " 0.6485\n",
      " 0.6486\n",
      " 0.6492\n",
      " 0.6495\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6498\n",
      " 0.6493\n",
      " 0.6478\n",
      " 0.6487\n",
      " 0.6481\n",
      " 0.6490\n",
      " 0.6502\n",
      " 0.6489\n",
      " 0.6494\n",
      " 0.6486\n",
      " 0.6496\n",
      " 0.6484\n",
      " 0.6484\n",
      " 0.6497\n",
      " 0.6490\n",
      " 0.6496\n",
      " 0.6490\n",
      " 0.6493\n",
      " 0.6494\n",
      " 0.6481\n",
      " 0.6497\n",
      " 0.6486\n",
      " 0.6500\n",
      " 0.6497\n",
      " 0.6487\n",
      " 0.6498\n",
      " 0.6485\n",
      " 0.6484\n",
      " 0.6488\n",
      " 0.6501\n",
      " 0.6488\n",
      " 0.6498\n",
      " 0.6498\n",
      " 0.6486\n",
      " 0.6481\n",
      " 0.6484\n",
      " 0.6508\n",
      " 0.6493\n",
      " 0.6494\n",
      " 0.6495\n",
      " 0.6489\n",
      " 0.6498\n",
      " 0.6494\n",
      " 0.6486\n",
      " 0.6489\n",
      " 0.6488\n",
      " 0.6490\n",
      " 0.6493\n",
      " 0.6491\n",
      " 0.6485\n",
      " 0.6482\n",
      " 0.6484\n",
      " 0.6495\n",
      " 0.6494\n",
      " 0.6500\n",
      " 0.6495\n",
      " 0.6495\n",
      " 0.6492\n",
      " 0.6492\n",
      " 0.6488\n",
      " 0.6487\n",
      " 0.6491\n",
      " 0.6496\n",
      " 0.6500\n",
      " 0.6491\n",
      " 0.6493\n",
      " 0.6490\n",
      " 0.6489\n",
      " 0.6495\n",
      " 0.6493\n",
      " 0.6499\n",
      " 0.6493\n",
      " 0.6492\n",
      " 0.6486\n",
      " 0.6484\n",
      " 0.6486\n",
      " 0.6487\n",
      " 0.6501\n",
      " 0.6487\n",
      " 0.6495\n",
      " 0.6484\n",
      " 0.6495\n",
      " 0.6490\n",
      " 0.6501\n",
      " 0.6500\n",
      " 0.6487\n",
      " 0.6498\n",
      " 0.6484\n",
      " 0.6496\n",
      " 0.6503\n",
      " 0.6492\n",
      " 0.6497\n",
      " 0.6501\n",
      " 0.6487\n",
      " 0.6490\n",
      " 0.6492\n",
      " 0.6491\n",
      " 0.6499\n",
      " 0.6497\n",
      " 0.6491\n",
      " 0.6496\n",
      " 0.6483\n",
      " 0.6483\n",
      " 0.6494\n",
      " 0.6485\n",
      " 0.6495\n",
      " 0.6493\n",
      " 0.6491\n",
      " 0.6496\n",
      " 0.6484\n",
      " 0.6488\n",
      " 0.6484\n",
      " 0.6490\n",
      " 0.6498\n",
      " 0.6499\n",
      " 0.6494\n",
      " 0.6488\n",
      " 0.6486\n",
      " 0.6483\n",
      " 0.6489\n",
      " 0.6487\n",
      " 0.6503\n",
      " 0.6497\n",
      " 0.6499\n",
      " 0.6496\n",
      " 0.6487\n",
      " 0.6493\n",
      " 0.6488\n",
      " 0.6485\n",
      " 0.6493\n",
      " 0.6500\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6485\n",
      " 0.6482\n",
      " 0.6488\n",
      " 0.6486\n",
      " 0.6488\n",
      " 0.6496\n",
      " 0.6488\n",
      " 0.6493\n",
      " 0.6493\n",
      " 0.6486\n",
      " 0.6496\n",
      " 0.6484\n",
      " 0.6502\n",
      " 0.6487\n",
      " 0.6490\n",
      " 0.6488\n",
      " 0.6503\n",
      " 0.6484\n",
      " 0.6490\n",
      " 0.6491\n",
      " 0.6495\n",
      " 0.6484\n",
      " 0.6494\n",
      " 0.6498\n",
      " 0.6496\n",
      " 0.6498\n",
      " 0.6491\n",
      " 0.6495\n",
      " 0.6488\n",
      " 0.6484\n",
      " 0.6489\n",
      " 0.6499\n",
      " 0.6491\n",
      " 0.6489\n",
      " 0.6480\n",
      " 0.6497\n",
      " 0.6489\n",
      " 0.6486\n",
      " 0.6495\n",
      " 0.6504\n",
      " 0.6499\n",
      " 0.6494\n",
      " 0.6483\n",
      " 0.6490\n",
      " 0.6495\n",
      " 0.6489\n",
      " 0.6486\n",
      " 0.6498\n",
      " 0.6497\n",
      " 0.6494\n",
      " 0.6486\n",
      " 0.6492\n",
      " 0.6492\n",
      " 0.6486\n",
      " 0.6483\n",
      " 0.6498\n",
      " 0.6484\n",
      " 0.6487\n",
      " 0.6488\n",
      " 0.6485\n",
      " 0.6497\n",
      " 0.6487\n",
      " 0.6487\n",
      " 0.6495\n",
      " 0.6504\n",
      " 0.6491\n",
      " 0.6484\n",
      " 0.6484\n",
      " 0.6497\n",
      " 0.6487\n",
      " 0.6490\n",
      " 0.6499\n",
      " 0.6482\n",
      " 0.6487\n",
      " 0.6501\n",
      " 0.6480\n",
      " 0.6495\n",
      " 0.6496\n",
      " 0.6493\n",
      " 0.6497\n",
      " 0.6498\n",
      " 0.6483\n",
      " 0.6486\n",
      " 0.6484\n",
      " 0.6493\n",
      " 0.6499\n",
      " 0.6499\n",
      " 0.6479\n",
      " 0.6484\n",
      " 0.6484\n",
      " 0.6493\n",
      " 0.6490\n",
      " 0.6491\n",
      " 0.6492\n",
      " 0.6482\n",
      " 0.6493\n",
      " 0.6501\n",
      " 0.6482\n",
      " 0.6502\n",
      " 0.6500\n",
      " 0.6487\n",
      " 0.6490\n",
      " 0.6496\n",
      " 0.6503\n",
      " 0.6498\n",
      " 0.6490\n",
      " 0.6486\n",
      " 0.6491\n",
      " 0.6497\n",
      " 0.6487\n",
      " 0.6492\n",
      " 0.6493\n",
      " 0.6498\n",
      " 0.6496\n",
      " 0.6482\n",
      " 0.6483\n",
      " 0.6506\n",
      " 0.6493\n",
      " 0.6481\n",
      " 0.6491\n",
      " 0.6500\n",
      " 0.6484\n",
      " 0.6492\n",
      " 0.6488\n",
      " 0.6483\n",
      " 0.6483\n",
      " 0.6480\n",
      " 0.6492\n",
      " 0.6502\n",
      " 0.6497\n",
      " 0.6487\n",
      " 0.6486\n",
      " 0.6491\n",
      " 0.6488\n",
      " 0.6489\n",
      " 0.6491\n",
      " 0.6485\n",
      " 0.6486\n",
      " 0.6492\n",
      " 0.6486\n",
      " 0.6488\n",
      " 0.6487\n",
      " 0.6492\n",
      " 0.6497\n",
      " 0.6491\n",
      " 0.6482\n",
      " 0.6492\n",
      " 0.6491\n",
      " 0.6497\n",
      " 0.6487\n",
      " 0.6495\n",
      " 0.6484\n",
      " 0.6488\n",
      " 0.6488\n",
      " 0.6496\n",
      " 0.6490\n",
      " 0.6485\n",
      " 0.6493\n",
      " 0.6484\n",
      " 0.6485\n",
      " 0.6487\n",
      " 0.6489\n",
      " 0.6496\n",
      " 0.6490\n",
      " 0.6496\n",
      " 0.6491\n",
      " 0.6491\n",
      " 0.6486\n",
      " 0.6496\n",
      " 0.6489\n",
      " 0.6482\n",
      " 0.6489\n",
      " 0.6484\n",
      " 0.6486\n",
      " 0.6488\n",
      " 0.6493\n",
      " 0.6503\n",
      " 0.6490\n",
      " 0.6488\n",
      " 0.6495\n",
      " 0.6503\n",
      " 0.6483\n",
      " 0.6486\n",
      " 0.6495\n",
      " 0.6488\n",
      " 0.6498\n",
      " 0.6484\n",
      " 0.6493\n",
      " 0.6490\n",
      " 0.6493\n",
      " 0.6482\n",
      " 0.6485\n",
      " 0.6492\n",
      " 0.6492\n",
      " 0.6482\n",
      " 0.6501\n",
      " 0.6489\n",
      " 0.6487\n",
      " 0.6503\n",
      " 0.6497\n",
      " 0.6482\n",
      " 0.6494\n",
      " 0.6492\n",
      " 0.6502\n",
      " 0.6495\n",
      " 0.6488\n",
      " 0.6491\n",
      " 0.6493\n",
      " 0.6492\n",
      " 0.6488\n",
      " 0.6493\n",
      " 0.6492\n",
      " 0.6499\n",
      " 0.6486\n",
      " 0.6502\n",
      " 0.6487\n",
      " 0.6492\n",
      " 0.6494\n",
      " 0.6498\n",
      " 0.6489\n",
      " 0.6497\n",
      " 0.6485\n",
      " 0.6493\n",
      " 0.6494\n",
      " 0.6493\n",
      " 0.6489\n",
      " 0.6503\n",
      " 0.6489\n",
      " 0.6502\n",
      " 0.6484\n",
      " 0.6492\n",
      " 0.6489\n",
      " 0.6492\n",
      " 0.6484\n",
      " 0.6498\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6496\n",
      " 0.6494\n",
      " 0.6492\n",
      " 0.6488\n",
      " 0.6494\n",
      " 0.6493\n",
      " 0.6491\n",
      " 0.6498\n",
      " 0.6498\n",
      " 0.6487\n",
      " 0.6486\n",
      " 0.6500\n",
      " 0.6501\n",
      " 0.6496\n",
      " 0.6498\n",
      " 0.6483\n",
      " 0.6482\n",
      " 0.6487\n",
      " 0.6502\n",
      " 0.6499\n",
      " 0.6500\n",
      " 0.6483\n",
      " 0.6487\n",
      " 0.6488\n",
      " 0.6498\n",
      " 0.6484\n",
      " 0.6490\n",
      " 0.6486\n",
      " 0.6486\n",
      " 0.6491\n",
      " 0.6493\n",
      " 0.6483\n",
      " 0.6495\n",
      " 0.6484\n",
      " 0.6489\n",
      " 0.6482\n",
      " 0.6486\n",
      " 0.6500\n",
      " 0.6498\n",
      " 0.6495\n",
      " 0.6492\n",
      " 0.6496\n",
      " 0.6487\n",
      " 0.6504\n",
      " 0.6490\n",
      " 0.6485\n",
      " 0.6498\n",
      " 0.6497\n",
      " 0.6487\n",
      " 0.6490\n",
      " 0.6498\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6500\n",
      " 0.6481\n",
      " 0.6483\n",
      " 0.6492\n",
      " 0.6485\n",
      " 0.6499\n",
      " 0.6499\n",
      " 0.6498\n",
      " 0.6496\n",
      " 0.6494\n",
      " 0.6483\n",
      " 0.6488\n",
      " 0.6495\n",
      " 0.6482\n",
      " 0.6488\n",
      " 0.6491\n",
      " 0.6490\n",
      " 0.6493\n",
      " 0.6488\n",
      " 0.6494\n",
      " 0.6483\n",
      " 0.6482\n",
      " 0.6488\n",
      " 0.6478\n",
      " 0.6487\n",
      " 0.6493\n",
      " 0.6496\n",
      " 0.6492\n",
      " 0.6502\n",
      " 0.6488\n",
      " 0.6506\n",
      " 0.6496\n",
      " 0.6500\n",
      " 0.6482\n",
      " 0.6495\n",
      " 0.6489\n",
      " 0.6496\n",
      " 0.6490\n",
      " 0.6504\n",
      " 0.6485\n",
      " 0.6492\n",
      " 0.6484\n",
      " 0.6492\n",
      " 0.6496\n",
      " 0.6498\n",
      " 0.6480\n",
      " 0.6488\n",
      " 0.6478\n",
      " 0.6488\n",
      " 0.6488\n",
      " 0.6485\n",
      " 0.6498\n",
      " 0.6488\n",
      " 0.6488\n",
      " 0.6487\n",
      " 0.6481\n",
      " 0.6496\n",
      " 0.6481\n",
      " 0.6483\n",
      " 0.6487\n",
      " 0.6498\n",
      " 0.6493\n",
      " 0.6501\n",
      " 0.6486\n",
      " 0.6495\n",
      " 0.6482\n",
      " 0.6502\n",
      " 0.6491\n",
      " 0.6488\n",
      " 0.6489\n",
      " 0.6501\n",
      " 0.6497\n",
      " 0.6488\n",
      " 0.6490\n",
      " 0.6497\n",
      " 0.6499\n",
      " 0.6494\n",
      " 0.6491\n",
      " 0.6492\n",
      " 0.6492\n",
      " 0.6483\n",
      " 0.6485\n",
      " 0.6493\n",
      " 0.6501\n",
      " 0.6493\n",
      " 0.6485\n",
      " 0.6493\n",
      " 0.6483\n",
      " 0.6495\n",
      " 0.6488\n",
      " 0.6496\n",
      " 0.6487\n",
      " 0.6487\n",
      " 0.6488\n",
      " 0.6486\n",
      " 0.6489\n",
      " 0.6500\n",
      " 0.6491\n",
      " 0.6480\n",
      " 0.6482\n",
      " 0.6489\n",
      " 0.6483\n",
      " 0.6483\n",
      " 0.6484\n",
      " 0.6481\n",
      " 0.6490\n",
      " 0.6489\n",
      " 0.6502\n",
      " 0.6488\n",
      " 0.6501\n",
      " 0.6498\n",
      " 0.6491\n",
      " 0.6484\n",
      " 0.6499\n",
      " 0.6497\n",
      " 0.6491\n",
      " 0.6498\n",
      " 0.6495\n",
      " 0.6488\n",
      " 0.6503\n",
      " 0.6493\n",
      " 0.6502\n",
      " 0.6492\n",
      " 0.6501\n",
      " 0.6491\n",
      " 0.6483\n",
      " 0.6483\n",
      " 0.6480\n",
      " 0.6496\n",
      " 0.6490\n",
      " 0.6490\n",
      " 0.6500\n",
      " 0.6486\n",
      " 0.6484\n",
      " 0.6482\n",
      " 0.6485\n",
      " 0.6490\n",
      " 0.6496\n",
      " 0.6510\n",
      " 0.6485\n",
      " 0.6493\n",
      " 0.6483\n",
      " 0.6499\n",
      " 0.6488\n",
      " 0.6490\n",
      " 0.6492\n",
      " 0.6489\n",
      " 0.6487\n",
      " 0.6496\n",
      " 0.6494\n",
      " 0.6487\n",
      " 0.6488\n",
      " 0.6492\n",
      " 0.6494\n",
      " 0.6481\n",
      " 0.6499\n",
      " 0.6484\n",
      " 0.6486\n",
      " 0.6492\n",
      " 0.6497\n",
      " 0.6496\n",
      " 0.6488\n",
      " 0.6491\n",
      " 0.6490\n",
      " 0.6501\n",
      " 0.6482\n",
      " 0.6495\n",
      " 0.6492\n",
      " 0.6488\n",
      " 0.6486\n",
      " 0.6494\n",
      " 0.6492\n",
      " 0.6492\n",
      " 0.6485\n",
      " 0.6496\n",
      " 0.6488\n",
      " 0.6481\n",
      " 0.6488\n",
      " 0.6494\n",
      " 0.6489\n",
      " 0.6482\n",
      " 0.6491\n",
      " 0.6487\n",
      " 0.6500\n",
      " 0.6496\n",
      " 0.6488\n",
      " 0.6488\n",
      " 0.6502\n",
      " 0.6482\n",
      " 0.6481\n",
      " 0.6502\n",
      " 0.6505\n",
      " 0.6502\n",
      " 0.6490\n",
      " 0.6486\n",
      " 0.6489\n",
      " 0.6493\n",
      " 0.6489\n",
      " 0.6485\n",
      " 0.6485\n",
      " 0.6498\n",
      " 0.6498\n",
      " 0.6490\n",
      " 0.6498\n",
      " 0.6498\n",
      " 0.6485\n",
      " 0.6502\n",
      " 0.6482\n",
      " 0.6499\n",
      " 0.6492\n",
      " 0.6501\n",
      " 0.6487\n",
      " 0.6489\n",
      " 0.6486\n",
      " 0.6484\n",
      "[torch.FloatTensor of size 759x1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewook/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([759])) that is different to the input size (torch.Size([759, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xy = np.loadtxt('./Data/diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x = Variable(tc.from_numpy(xy[:,0:-1])) # x: 759*8\n",
    "y = Variable(tc.from_numpy(xy[:,-1])) # y: 759*1\n",
    "\n",
    "class Model(tc.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = tc.nn.Linear(8,6) # Layer 1\n",
    "        self.l2 = tc.nn.Linear(6,4) # Layer 2\n",
    "        self.l3 = tc.nn.Linear(4,1) # Layer 3\n",
    "        self.sigmoid = tc.nn.functional.sigmoid\n",
    "        #self.sigmoid = tc.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x)) # Layer 1\n",
    "        out2 = self.sigmoid(self.l2(out1)) # Layer 2\n",
    "        out3 = self.sigmoid(self.l3(out2)) # Layer 3\n",
    "        return out3\n",
    "        \n",
    "model = Model()\n",
    "\n",
    "criterion = tc.nn.BCELoss(size_average=True)\n",
    "optimizer = tc.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(model(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
